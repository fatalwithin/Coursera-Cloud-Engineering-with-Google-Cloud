## Using GCP

There are four ways you can interact with GCP, and we'll talk about each in turn. There's the Google Cloud Platform console or GCP console, Cloud Shell and the Cloud SDK, the APIs and the Cloud mobile app. The GCP console provides a web-based graphical User Interface that you access through console.cloud.google.com For example, you can view your Virtual Machines and their details as shown on the top. If you prefer to work in a terminal window, the Cloud SDK provides the gcloud command line tool. For example, you can list your Virtual Machines and their details as shown on the bottom with the gcloud compute instances list command. GCP, also provides Cloud Shell which is a browser-based interactive shell environment for GCP that you can access from the GCP console. Cloud Shell is a temporary Virtual Machine with five gigabytes of persistent disk storage that has the Cloud SDK pre-installed. Throughout this course, you will apply what you learn in different labs. These labs will have instructions to use the GCP console such as on the navigation menu click ''Compute Engine > VM instances''. Let me dissect these instructions. First, within the GCP console, you will click on the icon with the three horizontal lines, which is the navigation menu as shown on the left. This opens a menu as shown on the right. All of the major products and services are listed on this menu. Then within the menu, hover over Compute Engine to open a sub-menu. Finally, click on VM instances on the sub-menu. You will get more comfortable with these instructions and the GCP console as you work on labs. Now, labs will also use command line instructions. You will enter these instructions either in Cloud Shell or an SSH terminal by simply copying and pasting them. In some cases, you will have to modify these commands, for example when choosing a globally unique name for a Cloud Storage bucket. In addition to the Cloud SDK, you can also use Client Libraries that enable you to easily create and manage resources. GCP Client Libraries expose APIs for two main purposes. App APIs provide access to services, and they're optimized for supported languages such as Node. js or Python. Admin APIs offer functionality for resource management. For example, you can use Admin APIs if you want to build your own automated tools. The Cloud mobile app is another way to interact with GCP. It allows you to manage GCP services from your Android or iOS device. For example, you can start, stop an SSH into Compute Engine instances, and see logs from each instance. You can also set up customizable graphs showing key metrics such as CPU usage, network usage, requests per seconds, and server errors. The app even offers alerts and incident management, and allows you to get up-to-date billing information for your projects, and get billing alerts for projects that are going over budget. You can download the Cloud mobile app from Google Play or from the App Store.

### Lab 1

In this lab, you created a Cloud Storage bucket using both the GCP Console and Cloud Shell within GCP. The GCP Console can do things Cloud Shell can't and vice versa. For example, the GCP Console can keep track of the context of your configuration activities. It can use the Cloud API to determine from the current system state, what options are valid, and it can perform repetitive or more leveraged activities on your behalf. Cloud Shell in contrast offers detailed and precise control, and through its commands, a way to script and automate activities. However, don't think of the constant Cloud Shell as alternatives, think about it as one extremely flexible and powerful interface. You can stick around for our lab walkthrough. But remember, that GCP is user interface can change, so your environment might look slightly different. So here we are in the GCP Console and the first thing we're going to do is create a bucket using the GCP Console. So to do that, I'm going to use the navigation menu which is the icon up here in the top left corner, and I'm currently scroll down to Storage which is here, and click on Browser. What we want to do is create a bucket, so I'm going to click the Create bucket. The first thing we need to do is define a name, and now this name needs to be a globally unique name. So you could for example use your Qwiklab's project ID here, so that's what I'll do, copy that and paste it in there. The instructions just say to create, you could also choose a change the default storage class is currently set to multi-regional. We'll talk more about that in a later module. You can control the access to the objects and there are even some advanced settings around encryption. So I'm just going to go ahead and click Create.
You can see that this now has created a bucket and here we see the bucket ID or the name. So now we're going to access Cloud Shell. Then what we're going to do this we're going to click this button up here on the right corner, says Activate Cloud Shell, and then it will prompt you to start clutches, so we'll click that as well.
You can see that's coming up here. You could actually expand this and open this in a new tab, or you could realigned this to get a little bit more real estate in here. So we created a bucket using the GCP Console, now we're going to repeat the same using Cloud Shell. So I'm going to go ahead and copy the command from the lab instructions and paste it in here. Another command has the bucket name here in brackets and we want to change that. So this again has to be a globally unique name. So what we could do is we could again grab the ID of our project and maybe just add something to it. We could just add -shell to say that this is the one that we created from Cloud Shell. So the command is gsutil, these are the commands for Cloud Storage and mb is the make bucket command. You'll see that it has created that here, and we can see if we navigate in the GCP Console back to buckets, that we now have two buckets in here. So we're able to create both of those. So there are other Cloud Shell features that we can explore here. So while we're in Cloud Shell, we can click these three dots over here and get some more options. One of which is we can upload a file, and if I click that, I'm just present it with my browser, and I could for example, select this text file and click Open. We see that it's being uploaded and now that has finished, and then I can use the ls command to list that file. So here's that file. There's also a read me already in there. Then we could copy that now, that file to the bucket that we have. So there's a command for that also in the lab instructions. So again, we're working with Cloud Storage. So gsutil is going to be the command and CP to copy. We're going to give the name of the file, so MyFile.txt, and then we want to get to that Cloud Storage bucket. So we could choose either of the two buckets we've created. Why don't we choose the one we create a from Cloud Shell, paste it in there, and then it's telling us that it's copying over the files. If we now go into that, we can see that now that file is in there. The file doesn't contain anything, so that's why it is that size. Then we could also go ahead and close Cloud Shell, and do some other activities. Task 5 of the lab goes into creating a persistent state in Cloud Shell. So you could open Cloud Shell and we could list for example, all the variable regions with the G loud command that's listed in there, G Cloud compute regions list, and from these regions we can now select a region and store that in an environment variable. So let's take the command from the lab instructions and for class region equals, and let's say for example I pick the US Central 1 region, could paste that in there, store it, and then I could verify that with the echo command, just running that and it's not telling me that that is stored in there. The other thing we could do is we could expand this a little bit, we could also create a folder in here with the MK direct command, and now we could create a configuration file, and then we can append the environment variable that we just created to to that file.
Then we could add another one for example, we could also store our project ID. So I can put that in there, grab my project ID, copy that, and store that in the environment variable, and then I run the command from the lab instructions to also append the value of the project ID to my environment variable and the configuration file. Then I can just verify all of that and make sure that that's been stored. So this gives us a method to create environment variables and easily recreate them as Cloud Shell is cycled. However, you will still need to remember to issue this source command each time Cloud Shell is opened. So let's modify the.profilefile so that the source command is issued automatically anytime a terminal Cloud Shell is opened. So we're going to close and reopen Cloud Shell. So let me do that, close it and then reopen it,
and then I'm going to paste the echo command again. We see that it's not outputting anything, so that command is coming out down. So let's modify that.profilefile using nano, and at the end of that file, let's go all the way to the bottom. We'll go into paste in sourceinfraclassconfig, and then we're going to save that file to profile, and then exit. Then let's verify that we are able to get that environment variable, that is project ID. So that's currently not in there, that is because I haven't restarted it, propagates run when I restart, sorry for that. So let me close it, let me reopen it, and then let's verify. There we go. So now we can see that expected value and that's because we edit the d.profilefile. That's it. So we've leveraged in this lab, the GCP Console, we created a Storage bucket, we also created a Storage bucket using Cloud Shell, and then we looked into some features run Cloud Shell in terms of uploading files, than copying those files to the Storage bucket, and even at the end configuring the profile and setting some environment variables. That's the end of the lab.

### Lab 2

In this lab, you are able to launch a complete continuous integration solution in a few minutes. You demonstrated that you had user access through the Jenkins UI and that you had administrative access control over Jenkins by using SSH to connect to the VM, where the service is hosted, and by stopping and then restarting the services. Many of the activities that occurred in that lab were nearly transparent, and they use resources and methods that you learn about in the rest of this course. Examples of this include; the acquisition and configuration of a network IP address, the provisioning of a virtual machine instance along with the installation of software on that machine, and the passing of default state information from the environment during the setup process. You can stick around for a lab walk-through, but remember that user interface can change. So you're environment might look slightly different. So here I am in the GCP Console, and the first thing I want to do is navigate to the Marketplace. So up here, I've already clicked on the Navigation menu and Marketplace is pretty much on top. So I'm going click on that. Now, I want to search for Jenkins. Specifically, the one that's certified by Bitnami. So I can just directly paste that in the search address here. Here we go. This is the one I'm looking for. So I'm going to click on that. Now, I can read all about this. There's an overview. It doesn't mean that function Compute Engine, uses a single virtual machine, when it was last updated. It talks about all the packages, the operating system. If I scroll down, I can learn more about the pricing. There's obviously, pricing associate with the VM instance itself. It does not have a usage fee. If it did, that would be displayed here, and you'd be billed for all of that together. There's a standard discharge, and then there's the sustained use discount, which we'll learn more about in a later module. So once I'm happy with all that and I've read through, I can go ahead and click on "Launch on Compute Engine". Now, it's going to present me with an interface here, where I could change the name, the zone, the Machine tab, a lot of other settings that are very similar to configuring a virtual machine. I can again, see all the Software, Terms of Service, the cost one more time. Once I'm ready to go, I can click "I accept the Terms of Services", and click "Deploy".
So now, I'm actually in a different interface. This is Deployment Manager, we'll learn about this later in the course series, but the interesting thing now, is I can see the setup process. So there is an actual file here that has all the configuration in a ginger file. There is a VM that's being created. There are two firewall rules that are created. TCP for port 80 and 443. So that's HTTP and HTTPS. I can wait for this machine to now come up. There's also some software configuration. I can again, learn about all the software that is installed here. I can click on the VM instance to get more information about it. We can see the VMs instance is up, the firewalls are up. So the last thing that's happening here is the software is being configured.
I can even learn more about that software. Here, I already clicked on that. So these are again, all the different versions that we can get to and engage. Once this is running, this table up here will be populated, all currently pending because this is still being initialized. Here, we can see that the instance is now ready. So there are a couple different things we could do. We have an admin user, as well as a password. So we can copy that. We could click on "Visit The Site", and this is going to open that in a new tab, that's navigating us to the external IP address. It's going to load, let's see it's the starting. It's part of the service itself, it's still getting ready to work. So you can see that the software in the background on the instance is installed, but it also needs to launch. So that itself can take some time too, and now it's up and running. I can put my username in and I can put the password in. I can click "Sign In".
Here, I should be asked to customize Jenkins. There'll be some suggested plug-ins that I can install. Once I've done that, I can restart the instance. Deployment Manager and the G Suite Marketplace, will also give you some time some suggests next steps. For example, this password up here, it's just temporary. So we could go change that. The other thing we could do is we could assign a static external IP address so that when you visit the site, it's always going to be the same IP address, and that really helps if you have a DNS setup for this instance. If I go back here, I can click that I want to install the suggested plug-ins. It's going to do that. It's going to tell me where that instance is. I can save and finish, and I can go start using it. They should again now restart service. So here we are. So I can explore this a little bit. I could manage Jenkins itself. There are lots of different actions that I could perform here.
I could also now, further administer the service if I go back to the Console. I'm looking at this deployment here. I'm looking Jenkins-1, I could actually SSH now to this instance. So let me click that button. That's going to establish now, an SSH session to the service. I can then actually shut down all the services by copying the command that's in the lab instructions. So let me just paste that in here and run that. If we go back to the Jenkins UI and refresh that page, we'll see that it's gone. That is expected because I have gone ahead and I have restarted that service. So what I can do now, is I stop them, I can now restart it by running the Restart command in here. So let's grab that and paste that in here. Now, the service should come back up. We might have to refresh the page a couple times for that to happen. So let's just wait a couple seconds, refresh it and see if that's service comes back up. My tab name has changed to Start in Jenkins. So it looks like that service is already coming back up right now.
We can see that the service is getting ready right now. So at this point, we've completed all the task. I could now go back to the SSH session and exit out of here. Here, we see that Jenkins is back up and running. That's the end of the lab.

Let's explore projects which are the key organizer of infrastructure resources and relate these resources to billing accounts. Resources can only be created and consumed within projects in a way that projects isolate related resources from one another. I will demonstrate how to create and delete projects, and switch contexts between projects. Some of these actions cannot be performed in the Qwiklabs environment due to security restrictions. Therefore, I'm going to demonstrate them in my environment. So here I am in the GCP console. You can actually see this is a trial account and you can also create a trial account yourself if you would like to follow along with this. Essentially, what I'm going to do first is go ahead and create a project. So I'm going to click up on my product name up here, and there's this icon up here to create a new project, so let me go click that. Now, the one thing I want to do is, I want to define a project name. So let me just say my new project. You can see that it automatically creates the project ID and project ID is going to be unique versus my name is really not so unique. So let me click "Create" on that. It is now telling me here that is going to create that project. I can follow along with that here in the notification pane. One thing to notice is, when you create a new project, that some of the services that you're going to use may not be initially available. So here, I now have my new project. I could now switch projects. So if I go to my home, for example, I see here the project itself, I could go to the project settings, I could shut that down, or I could switch to a different one. So let me actually change up here to this new project that I created. You go in there and let's follow the process for shutting that down. So I'm going to click on "Shutdown", I wants to make sure that I really want to do that. It's telling me a little bit about what's going to happen when I do this. Specifically, all building in traffic serving will stop, but the shutdown is actually scheduled. So it will take 30 days, and this is in case that you want up maybe undo this. So I need to just retype my project ID, and I can actually copy and paste it in here, and I can click ''Shutdown'', and it should now give me a notification. So here it's telling me when exactly it's going to shut this down, and I can click ''OK'' on that. So now, this is being scheduled for shutdown. So now, I can go back and obviously want to grab in project, it's automatically put me in the sight. Alternatively, if I go home, you'll see that I also have an option up here. It's telling me, hey, you really need to select a project. So lots of different ways to go about. So I could click on that and select a project. Now, I want to show in a second how we can also move switched projects on screen Cloud Shell. So let's actually go ahead and create another project. Let's just call this My Second Project.
We can create that as well. They'll start in the background for us. So what I want to do now, as I said I want to go to Cloud Shell. So if I go up here on the right corner, it's Activate Cloud Shell. I'll just click on that. It doesn't ask me if you want to start Cloud Shell because I've already been using Cloud Shell with this user. So it's also telling me that I haven't used my Cloud Shell in awhile, so it has to unarchive my disk and that's going to take a little bit of time. But once that's up, we can actually go use gcloud config list command and we can paste it in, and it's going to give us more information about the configuration that we currently have. That will include the project that we currently have selected. We can actually see the project right here. This is the project I'm working on right now. So if I paste in, I automatically copy that when I clicked on it. So I want to instead type in here gcloud config list. So here, we get some more information. I can also use the grep command in here to directly got my project and there we see this is the project that we're currently using. I could actually now even changed the focus of my GCP console to this new project. You'll see if I run this command again, my focus of Cloud Shell is still focused on this other project that I had. So one thing we could do now, is we could store the project ID maybe in an environment variable and then we could maybe set it so we could swap back and forth. So let me get the project ID, it's right here. I'm going to maybe just store that in an environment variable. Let's just call that my project ID1. So let me grab the project ID, copy that, paste it in there. So now, I have that stored and now I could use the gcloud config set project to define an action to change the project ID. Now, you can see that I have that other project ID listed here. So I can actually see that, and I could also now use it the same gcloud config list command and grep the project, and you'll see that now I'm working with different project. That's how easy it is to create and delete projects, and switch contexts between projects.

## Virtual Private Cloud

In this module, we will be covering virtual networks. GCP uses a software defined network, that is built on a global fiber infrastructure. This infrastructure makes GCP one of the world's largest and fastest networks. Thinking about resources as services instead of as hardware, will help you understand the options that are available, and their behavior. In this module, we start by introducing Virtual Private Cloud or VPC, which is Google's managed networking functionality, for Euro Cloud Platform resources. Then, we dissect networking into its fundamental components. Which are projects, networks, subnetworks, IP addresses, routes, and firewall rules, along with network pricing. Next, you will explore Google Cloud's network structure in a lab, by creating networks of many different varieties, and exploring the network relationships between them. After that, we will look at common network designs. This map represents Google Cloud. On a high level, Google Cloud consists of regions, which are the icons in blue, points of presence or PoPs, which are the dots in blue, a global private network, which is represented by the blue lines, and services. A region is a specific geographical location where you can run your resources. This map shows several regions that are currently operating, as well as future regions. Regions indicated with blue icons have three zones. Iowa is an exception, where the region called US-Central1 has four zones: US-Central1-A, US-Central1-B, US-Central1-C, and US-Central1-F. For up-to-date information on regions and zones, please refer to the documentation in the slides. The PoPs, are where Google's network is connected to the rest of the internet. Google Cloud can bring its traffic closer to its peers, because it operates an extensive global network of interconnection points. This reduces costs and provides users with a better experience. The network connects regions and PoPs, and is composed of a global network of fiber optic cables with several submarine cable investments. For more information about Google's networking infrastructure, please refer to these slides. Let's start by talking about GCPs network, and specifically Virtual Private Cloud or VPC.

With GCP, you can provision your GCP resources, connect them to each other, and isolate them from each other in a Virtual Private Cloud. You can also define fine-grained network and policies within GCP and between GCP and On-premises or other public Clouds. Essentially, VPC is a comprehensive set of Google managed networking objects, which we will explore in detail throughout this module. Let me give you a high-level overview of these objects. Projects are going to encompass every single service that you use including networks. Networks come in three different flavors; default, auto mode, and custom mode. Subnetworks allow you to divide or segregate your environment. Regions in zones represents Google's datacenters and they provide continuous Data Protection and high availability. VPC provides IP addresses for internal and external use along with granular IP address range selections. As for virtual machines, in this module, we will focus on configuring VM instances from a networking perspective. We'll also go over routes and firewall routes.

### Projects, networks, and subnetworks

Let's start exploring the VPC objects by looking at projects, networks, and subnetworks. Projects are the key organizer of infrastructure resources in GCP. A project associates objects and services with billing. Now, it's unique that projects actually contain entire networks. The default quota for each project is five networks but you can simply request additional quota using the GCP console. These networks can be shared with other projects or they can be peered with networks in other projects. Both of which we'll cover later in the Architecting with Google Compute Engine Core series. These networks do not have IP ranges but are simply a construct of all of the individual IP addresses and services within that network. GCP networks are global spending all available regions across the world that I showed earlier. So you can have one network that later exists anywhere in the world, Asia, Europe, Americas, all simultaneously. Inside a network you can segregate your resources with regional subnetworks. I just mentioned that there are different types of networks: default, auto, and custom. Let's explore these types of networks in more detail. Every project is provided with a default VPC network with presets subnets and firewall rules. Specifically a subnet is allocated for each region with non-overlapping sider blocks and firewall rules that allow ingress traffic from ICMP, RDP, and SSH traffic from anywhere, as well as ingress traffic from within the default network for all protocols and ports. In an Auto Mode network, one subnets from each region is automatically created within it. The default network is actually an auto mode network. These automatically created subnets uses set of predefined IP ranges with a /20 mask that can be expanded to a /16. All of these subnets fit within the 10.128.0.0/9 cider block. Therefore, as new GCP regions become available, new subnets and dose regions are automatically added to automotive networks using an IP range from that block. A Custom Mode network does not automatically create subnets. This type of network provides you with complete control over its subnets and IP ranges. You decide which subnets to create in regions you choose and using IP ranges you specify within the RFC 1918 address space. These IP ranges cannot overlap between subnets of the same network. Now, you can convert an auto mode network to a custom mode network to take advantage of the control that custom mode networks provide. However, this conversion is one way. Meaning that custom mode networks cannot be changed to auto mode nodes. So carefully review the consideration for auto mode networks to help you decide which type of network meets your needs. On this slide, we have an example of a project that contains five networks. All of these networks span multiple regions across the world as you can see on the right. Each network contains separate virtual machines: A, B, C, and D. Because VM's A and B are in the same network, Network 1, they can communicate using their internal IP address even though they are in different regions. Essentially your virtual machines even if they exist in different locations across the world, take advantage of Google's global fiber network. Those Virtual Machines appear as though they're sitting in the same rack, when it comes to a network configuration protocol. VM C and D however are not in the same network. Therefore by default these VM's must communicate using their external IP addresses even though they are in the same region. The traffic between VM C and D isn't actually touching the public Internet but is going through the Google edge routers. This has different billing and security ramifications that we will explore later. Because VM instances within a VPC network can communicate privately on a global scale, a single VPN can securely connect your on-premises network to a GCP network as shown in this diagram. Even though the two VM instances are in separate regions, US-West 1 and US-East 1, they leverage Google's private network to communicate between each other and to an on-premises network through a VPN gateway. This reduces cost and network management complexity. I mentioned that subnetworks work on a regional scale. Because a region contains several zones, subnetworks can cross zones. This slide has a region, Region 1 with two zones: zones A and B. Subnetworks can extend across these zones within the same region such as subnet-1. The subnet is simply an IP address range and you can use IP addresses within that range. Notice that the first and second addresses in the range 10.0.0.0 and 10.0.0.1 are reserved for the network and these subnets gateway respectively. This makes the first and second available addresses 10.0.0.2 and 10.0.0.3 which are assigned to the VM instances. The other reserved addresses in every subnets are the second-to-last address in the range and the last address which is reserved as the broadcast address. So to summarize, every subnet has four reserved IP addresses in its primary IP range. Now, even though the two Virtual Machines in this example are in different zones, they still communicate with each other using the same subnet IP address. This means that a single firewall rule can be applied to both VM's even though they are in different zones. Speaking of IP addresses of a subnet, Google Cloud VPC's let you increase the IP address space of any subnets without any workload shutdown or downtime. This diagram illustrates a network with subnets that have different subnet masks allowing for more instances in some subnets than others. This gives you flexibility and growth options to meet your needs but there are some things to remember. The new subnet must not overlap with other subnets in these same VPC network in any region. Also, the new subnets must stay inside the RFC 1918 address spaces. The new network range must be larger than the original which means the prefix length value must be a smaller number. In other words, you cannot undo an expansion. Now, auto mode subnets start with a /20 IP range. They can be expanded to a /16 IP range but no larger. Alternatively, you can convert the auto mode subnetwork to a custom mode subnetwork to increase IP range further. Also avoid creating large subnets. Overly large subnets are more likely to cause site arrange collisions when using multiple network interfaces and VPC network peering or when configuring a VPN or other connections to an on-premises network. Therefore, do not scale your subnet beyond what you actually need.

Let me show you how to expand a custom subnet within GCP. I've already created a custom subnet with a slash 29 mask. A slash 29 mask provides you with eight addresses. But of those, four are reserved by GCP, which leaves you with another four for your VM instances. Let's try to create another VM instance in this subnet. So here we are on the GCP console, and I have my four instances, and if I go into the network interface details here, you can see that these are part of a network and I have a subnet here, and if I drilled further into that you can see that I currently have a slash 29. So let's go back and try to create that other instance. Just going to click on Create Instance. I don't need a very large machine, I'm okay with the micro, and let's hit Create.
Ideally, we should be getting an error now about the fact that the IP space should have been exhausted, so we're just going to wait for that. You can also follow this along in the notification pane up here and see that it is trying to create that right now. So we're going to wait for that and see if we get an error here in a second. Once we have that, we're going to go ahead and expand the subnet. So here we can see that the instance creation has failed, I can hover over this and it's just telling me that the IP space of that subnet has been exhausted just as expected. We actually have a "Retry" button here as well as a notification pane. We're going to try to use that in a second once we expand the subnet to recreate that instance. Now, what's important is to note that all of these four instances are currently running. So we're not going to take any of these town during the subnet expansion. Now to expand the subnet, I could go to VPC networks through the navigation menu, or I can go back by clicking on nic0 here directly through the network interface details. So the subnet, this is what I want to change, so let me click the "Edit" button, and lets expand this all the way to a slash 23, and this is going to allow a lot of instances, actually over 500 instances. We're going to wait for this to update, and then we're going to head back and try to recreate that instance.
So we can also follow this process along right here. It's still saving, so we're going to just hang on tight here. It should just take a couple seconds.
All right. We see it's complete. Now, I still have that "Retry" button here to recreate that instance. So let me actually click that, and I can head back to Compute Engine to see if that is going to succeed. So here we are, instance five, it's being staged and will soon begin running. Let's see if this works out. We can see that already has now an internal IP address allocated now that we've expanded the subnet itself, and if I refresh this we can see that the instance is now created. That's how easy it is to expand a subnet in GCP without any workload shutdown or downtime

### IP Addresses 

Now that we covered GCP networks at a high level, let's go deeper by exploring IP addresses. In GCP, each virtual machine can have two IP addresses assigned. One of them is an internal IP address, which is going to be assigned via DHCP internally. Every VM that starts up and any service that depends on virtual machines gets an internal IP address. Example of such services are App Engine and Kubernetes Engine, which are explored in other courses. When you create a VM in GCP, it's symbolic name is registered with an internal DNS service that translates the name to the internal IP address. DNS is scoped to the network, so it you can translate web URLs and VM names of hosts in the same network, but it can't translate host names from VMs in a different network. The other IP address is the external IP address, but this one is optional. You can assign an external IP address if your device or your machine is externally facing. That external IP address can be assigned from a pool, making it ephemeral, or it can be assigned a reserved external IP address, making it static. If you preserve a static external IP address and do not assign it to a resource such as a VM instance or a forwarding rule, you are charged at a higher rate than for static and ephemeral external IP addresses that are in use.

I just mentioned that VMs can have internal and external IP addresses. Let's explore this in the GCP Console. So here I am on the Compute Engine page. What I'm going to do is just create a VM and walk through the process of choosing your internal and external IP address. So let me click Create. I can leave the name. You have obviously a selection of regions and zones you can choose, but I want to focus on the IP addresses. So let me go down to this option, expand management security networking sole tenancy. Let's focus on networking. Here at the network interface, I'm going to click the pencil icon. I could choose between two different networks. So if I had different networks, I could choose between them. That's not the case here. Then I have the primary or internal IP and external IP. So if we look at those options, you can see that I can use an ephemeral address either the one that's created automatically or I could custom select one. So within the range that I have here I could just type IP address. I could also reserve a static internal IP address. This is great if you want to keep that IP address for a longer time and we have similar options with the external IP address. But one of the big differences is that you can also just select none. So as I mentioned your instances don't need to have an external IP address. So let's just leave this as ephemeral. By the way, with the slash 20 here we have a lot of space in this IP range over 4,000 addresses. So we could definitely have that many instances. There are also limits of how many instances you can have per network. As of this recording is actually 15,000. So do keep that in mind you might have a very large IP range but that doesn't mean that you actually can create that many instances. That's a quota. There may also be actual limitations on physical hardware that's even available within a specific region or zone. So let me go ahead and create this instance. We're going to keep an eye on the internal and as well as the external IP address. Once the instance is created. Then we're also going to stop and start the instance to see if any of the IP address has changed. So here we can see the internal IP address. So that is definitely within that space that we just looked at. The external IP address obviously is within Google strange here and we could have reserved that, but this is an fMRL one. So let's actually test this out. I'm going to select the instance. I'm going to stop it. So it's telling me that it doesn't move in 90 seconds that might be forced. So if you had any shutdown scripts in here you want to make sure that they can actually complete within 90 seconds. So let's run through that. Remember this external IP address that we currently have here as well as the internal IP address.
So this is going to take it's time now. We can also click Refresh to keep an eye on this. But this will take about 90 seconds, and that's just to give your shutdown script enough time to perform any task to gracefully shut down this instance. So here we are, we can see the instance is stopped, the external IP address is gone. So now we're just going to startup that instance again. It's going to tell us it we're going to be build while it's running, that's fine. You can see that the internal IP address remained the same wildest instance stopped. So that has actually stayed for the time being. Now, while this instance spins up which we can by the way monitor the progress over here, we should see that we should be getting a new external IP address now because that was an ephemeral address. So here we can see the instance has started back up and we can see that the external IP address has changed. This demonstrates that every VM needs an internal IP address but external IP addresses are optional and by default, there are ephemeral.

### Mapping IP Addresses

Regardless of whether you use an ephemeral or static IP address, the external address is unknown to the OS of the VM. The external IP address is mapped to the VMs internal address transparently by VPC. I'm illustrating this here by running if config within a VM in TCP, which only returns the internal IP address. Let's explore this further by looking at DNS resolution for both internal and external addresses. Let's start with internal addresses, each in Instance has a host name that can be resolved to an internal IP address. This hostname is the same as the instance name. There's also an internal fully qualified domain name or fqdn for an instance that uses the format shown on the slide. If you delete and recreate an instance, the internal IP address can change. This change can disrupt connections from other compute engine resources, which must obtain the new IP address before they can connect again. However, the DNS name always points to specific instance no matter what the internal IP address is. Each instance has a metadata server that also acts as a DNS resolver for that instance. The metadata server handles all DNS queries for local network resources and routes all other queries to Google's public DNS servers for public name resolution. I previously mentioned that an instance is not aware of any external IP address assigned to it. Instead, the network stores a lookup table that matches external IP addresses with the internal IP addresses of the relevant instances. For more information including how to setup your own resolve on instances see the link section of this video. Now let's look at external addresses. Instances with external IP addresses can allow connections from hosts outside of the project. Users can do so directly using the external IP address. Public DNS records pointing to instances are not published automatically. However, admins can publish these using existing DNS servers. Domain and servers can be hosted on gcp using Cloud DNS. This is a managed service that's definitely worth considering, so let's explore them more detail. Cloud DNS is a scalable, reliable and managed authoritive domain name system or DNS service running on the same infrastructure as Google. Cloud DNS translate requests for domain names like google.com into IP addresses. Cloud DNS uses Google's Global Network of any cast name servers to serve your DNS zones from a download locations around the world providing lower latency and high availability for your users. High availability is very important because if you can't look up a domain name the internet might as well be down. That's why gcp offers a 100% up-time service level agreement or SLA for domains configured in cloud DNS. For more information about this SLA see the link section of this video. Cloud DNS lets you create and update millions of DNS records without the burden of managing your own DNS service and software. Instead, you use a simple user interface, command line face or API. For more information about cloud DNS see the link section of this video. Another networking feature of gcp is alias IP ranges, alias IP ranges that you assign a range of internal addresses as an alias to Virtual machines network interface. This is useful if you have multiple services running on a VM and you want to assign a different IP address to each service. In essence, you can configure multiple IP addresses representing containers or applications hosted in a VM. Without having to define a separate network interface. You just draw the alias IP range from the local subnets primary or secondary side arranges. This diagram provides a basic illustration of primary and secondary site arranges and VM Alias IP ranges. For more information about alias IP ranges see the link section of this video.

### Routes and firewall rules

So far you've learned about projects, networks, subnetworks, and IP addresses. Let's use what you learned to understand how GCP routes traffic. By default, every network has routes that let instances in a network send traffic directly to each other even across subnets. In addition, every network has a default route that directs packets to destinations that are outside the network. Although these routes cover most of your normal routing needs, you can also create special routes that overwrite these routes. Just creating a route does not ensure that your packet will be received by the specified next top. Firewall rules must also allow the packet. The default network has preconfigured firewall rules that allow all instances in the network to talk with each other. Manually created networks do not have such rules, so you must create them as you will experience in the first lab. Routes match packets by destination IP addresses. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, enabling traffic delivery from anywhere. Also, a route is created when a subset is created. This is what enables VM's on the same network to communicate. This slide shows a simplified routing table, but let's look at this in more detail. Each route in the routes collection may apply to one or more instances. A route applies to an instance if the network and instance tags match. If the network matches and there are no instance tags specified, the route applies to all instances in that network. Compute engine then uses the routes collection to create individual read-only routing tables for each instance. This diagram shows a massively scalable virtual router at the core of each network. Every virtual machine instance in the network is directly connected to this router, and all packets leaving a virtual machine instance are first handled at this layer before they are forwarded to the next hop. The virtual network router selects the next hop for a packet by consulting the routing table for that instance. GCP firewall rules to protect you virtual machine instances from unapproved connections both inbound and outbound known as ingress and egress respectively. Essentially, every VPC network functions as a distributed firewall. Although firewall rules are applied to the network as a whole, connections are allowed or denied at the instance level. You can think of the firewall as existing not only between your instances and other networks, but between individual instances within the same network. GCP firewall rules are stateful. This means that if a connection is allowed between a source and a target or a target at a destination, all subsequent traffic in either direction will be allowed. In other words, firewall rules allow bidirectional communication once a session is established. Also if for some reason all firewall rules in a network are deleted, there is still an implied deny all ingress rule and an implied allow all egress rule for the network. You can express your desired firewall configuration as a set of firewall rules. Conceptually, a firewall rule is composed of the following parameters: the direction of the rule. Inbound connections are matched against ingress rules only, and outbound connections are matched against egress rules only. The source of the connection for ingress packets or the destination of the connection for egress packets. The protocol and port of the connection where any rule can be restricted to apply to specific protocols only or specific combinations of protocols imports only. The action of the rule which is to allow or deny packets that match the direction, protocol port and source or destination of the rule. The priority of the rule which governs the order in which rules are evaluated. The first matching rule is applied. The rule assignment. By default all rules are assigned to all instances but you can assign certain rules to certain instances only. For more information on firewall rule components, please refer to the links section of this video. Let's look at some GCP firewall or use cases for both egress and ingress. Egress firewall rules control outgoing connections originated inside your GCP network. Egress allow rules allow outbound connections that match specific protocol ports and IP addresses. Egress deny rules prevent instances from initiating connections that match non permitted port protocol and IP range combinations. For egress firewall rules, destinations to which a rule applies may be specified using IP CIDR ranges. Specifically, you can use the destination ranges to protect from undesired connections initiated by a VM instance towards an external host as shown on the left. You can also use destination ranges to prevent undesired connections from internal VM instances to specific GCP CIDR ranges. This is illustrated in the middle, where a VM in a specific subnet is shown attempting to connect inappropriately to another VM within the same network. Ingress firewall rules protect against incoming connections to the instance from any source. Ingress allow rules allow specific protocol ports and IP ranges to connecting. The firewall prevents instances from receiving connections on non-permitted ports and protocols. Rules can be restricted to only affect particular sources. Source CIDR ranges can be used to protect an instance from undesired connections coming either from external networks or from GCP IP ranges. This diagram illustrates a VM receiving a connection from an external address, and another VM receiving a connection from a VM within the same network. You can control ingress connections from a VM instance by constructing inbound connection conditions using source CIDR ranges, protocols, or ports.

FW rules parameters:
- direction - inbound for ingress, outbound for egress
- source/destination - for ingress - IP address, source tags, service accounts. For egress - ranges of IP addresses
- protocol/port - combination of one and other
- action - allow/deny
- priority
- rule assignment - to all instances by default.

### Pricing

Before you apply what you just learned, let's talk about network pricing. It is important that you understand the circumstances in which you are built for GCP's network. This table, is from the Compute Engine documentation and it lists the price of each traffic type. First of all, egress or traffic coming into GCP's network is not charged, unless there is a resource, such as a load balancer that is processing egress traffic. Responses to request account as egress and are charged. The rest of this table, lists egress or traffic leaving a virtual machine. Egress traffic to the same zone, is not charged as long as that egress is through the internal IP address of an instance. Also egress traffic to Google products like YouTube, maps, drive, or traffic to a different GCP service within the same region, is not charged for. However, there is a charge for egress between zones in the same region, egress within a zone, if the traffic is through the external IP address of an instance, and egress between regions. As for the difference in egress traffic to the same zone, Compute Engine cannot determine the zone of a virtual machine through the external IP address. Therefore, this traffic is treated like egress between zones in the same region. Also there are some exceptions and pricing can always change. So refer to the documentation in the links section of these slides. Now, you are charged for static and ephemeral external IP addresses. This table, represents the external IP pricing for us-central1 as of this recording. You can see that if you reserve a static external IP address and do not assign it to a resource, such as a VM instance or a forwarding rule, you are charged at a higher rate and for static and ephemeral external IP addresses that are in use. Also external IP addresses on preemptible VMs, have a lower charge than for standard VM instances. Remember, pricing can always change, so please refer to the documentation link in the slides. Also I recommend using the GCP pricing calculator to estimate the cost of a collection of resources, because each GCP service has its own pricing model. The pricing calculator is a web-based tool, that you use to specify the expected consumption of certain services and resources, and it then provides you with an estimated cost. For example, you can specify a specific instance type, in a specific region along with 100 gigabytes of monthly egress traffic to Americas and EMEA. The pricing calculator then returns the total estimated cost. You can adjust the currency and time frame to meet your needs, and when you finish, you can e-mail the estimate or save it to a specific URL for future reference. To use the pricing calculator today, refer to the link in the slides.

All ingress, and also egress to GCP services or other Google products - not charged
What you should pay for:
- Egress between zones in same region - $0.01/GB
- Egress to the same zone to external IP - $0.01/GB
- Egress between regions in US/Canada - $0.01/GB
- Egress between regions in other countries - varies
- Static IP address - $0.01/h
- static/ephemeral IP address in use on standard VM instances - $0.004/h
- static/ephemeral IP address in use on preemptible VM instances - $0.002/h

### Lab Review: VPC Networking

In this lab, you explore the default network, and determine that you cannot create VM instances without a VPC network. So you created a new auto mode VPC network, with subnets, roots, firewall rules, and two VM instances, and tested connectivity for those VM instances. Because auto mode networks aren't recommended for production, you converted the auto mode network to a custom mode network. Next, you create two more custom mode VPC networks with firewall rules and VM instances using the GCP console, and the GCloud command line. Then you test the connectivity across VPC networks, which worked when you pinged external IP addresses, but not when you pinged internal IP addresses. VPC networks are, by default, isolated private networking domains. Therefore, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC peering, or a VPN connection. You can stay for a lab walk through. But remember, that GCP's user interface can change, so your environment might look slightly different. All right. So here I am in the GCP console. The first thing I'm going to do is I'm just going to explore the default network. So if I, on the left-hand, side click on the navigation menu, and scroll down to VPC network, we will see that this project has a default network. Every project has a default network. That is unless you have an organizational policy that prevents this default network from being created. But essentially, all the different projects that used through Qwiklabs will always have this. So in here, we can see we have a different subnet in each of the different regions. All of these are private IP addresses. I can also go to the routes, and these are established automatically with the networks. So we can see routes between subnets, as well as to the default route, to the Internet. We can even look at the firewall rules. The default network comes with some preset firewall rules to allow ICMP traffic from anywhere. RDP traffic, as well as SSH. Then also, all protocols imports within the network. So this is the range of the network. So we also allow all traffic from within the network itself. So let's go ahead and let's actually delete these firewall rules. I can just check them all right here and delete them. Let's just assume that we want to get rid of everything that's been created for us, and just create our own network instead. So I'm going to go ahead and delete these. I can look at the status up here. We can see that all four are being deleted. It'll update as each as being deleted. Once that is done which is now, I can head to the network, select the default network, and we're also just going to delete that entire network.
Once we delete this network, we should see that there should be no routes without a network because there's no use case for them. So let's just wait for the network to be deleted and then we'll verify that. So we can, again, see the progress bar up here, that's deleting, you can also hit refresh, and this should just take a couple seconds.
You can see that as I'm refreshing, some of the subnets are disappearing. It's actually just deleting them all the subnets first, and then it's getting rid of the network as a whole, because the network is really nothing else than just a combination of subnets. So all these subnets have to be deleted. There we go. They're all gone now. Now, it's just the network itself that is remaining.
If I go to routes, we should see that all the routes already gone, because without the subnets, there's really no need for the routes. If I go back to the network, we should see that any moment now the network itself also disappears. There we go. All right. So without a VPC network now, we shouldn't be able to create any VM instances, containers, or app engine application. Let's actually verify that. I'm going to go to the navigation menu, go to compute engine, and let's just try to create an instance, just going to click create. I'm going to leave everything as its default. If I go actually under networking, we should see that it's going to complain here. If I click on networking, that actually doesn't have a local network available. But let's just click create and see what happens, and it does indeed give us an error, and point out the fact that this tab has an issue. So we clearly cannot create an instance, because again, these instances live in networks, and without a network, we can't create it. So let's hit cancel, and what we're going to do now is we're going to create our own auto mode network. So I'm going to head back to VPC networks. You can pin, by the way, the services. So I'm just going to pin VPC network, compute engine, because we're going to be going back and forth between these. Then within VPC network, we're just now going to create our own network. I can give it a name. I'm going to use the same name that I have in lab instructions, which is My Network. Now I have the option of creating a custom or an automatic. Let's start off by creating an automatic network. So that's going to preset all the distance subnets for us in all the different regions that are available. You can scroll through those and see those all in here. They have a preset to side arrange. You can expand that side arrange later. But again, as an auto network, you don't define the actual IP address range. There are also firewall rules that are available. What's interesting here is you see that there's a deny-all ingress and allow all ingress firewall rule. So these are here by default, and they're actually. You can't even uncheck them. So these are actually with all networks that you create, and you can see that this has the highest party integer, which really means it's a lowest priority. So by default, all ingress traffic is denied, and all ingress traffic is allowed. Unless we create other firewalls to see differently. So if I check all these boxes, we're now allowing ingress traffic for these IP ranges, and these protocols imports. So let's go ahead and click create, and we're going to wait for that network to be created. Then we're going to look at the IP addresses for two of the different regions, and we're going to create instances in those regions, and verify that it's taking those IP addresses. So here, you can see the subnets already all populated here. I can monitor the progress also up here, but this is really done any second now. I'm actually going to start heading over to compute engine, and to create our instances. So let's click create. I'm going to give it a name mynet-us-vm. This is going to be in your central one, specifically, the Zone C. I don't really need a big machine. We're just doing some testing here. So let me just create a micro that reduces the cost a little bit, and I'm going to now click create.
Then we're going to repeat. I can close this panel over here. The same workflow and create an instance in Europe. So I'm going to grab the name from the lab instructions for that. I'm going to select the Europe West One region, specifically the Zone 1C. Again, a micro machine which is just a shared-core, and click create for that as well. We can see the US Central 1C machine is already up. We also see the internal IP address that has been provided. Again, there are some reserved IP addresses. The dot zero is reserved as well as the dot one. So in both of these ranges, the dot two is the first available address. Now, we can verify that these are part of the right subnet, if I click on nic0, I go to the network interface details. Here, we can see it's part of the sub-network. Now the sub-network, in this case, has the same name as the network because this is an auto network. Here, we can see that it's part of this range. So 1012800/20. Let's verify that, and that is correct. We are in there with a dot two, and let's verify that the other should be now a 10132.00/20. So again, click on nic0, go to the sub-network, and we can see that's true. You can also see here that this address is reserved for the gateway. All right. So that way, the dot two was really the first usable address within that range. So now, these are on the same network. So let's verify some connectivity between those. I'm going to grab the internal IP address of mynet-eu-vm, just copy that, and then we're going to SSH too this other instance. So again, these instances are in two separate regions but in the same network. So we should be able to ping these addresses now. So if I ping three times using the internal address, then we can see that this works. This works because we have that allow internal firewall rule that we selected earlier. I can actually repeat the same by using the name of the instance.
You can see that it's taking that name. It's actually has, here, the fully qualified domain name, and it's just using the IP address for that. So VPC networks have an internal DNS service that allows you to address instances by that DNS names, instead of their internal IP addresses. That's very useful because, well, this internal IP address could change, right? But the name is not going to change. So it's always good to be aware of that, that you can use the fully qualified domain name to ping those. All right. Now we can try this whole thing the other way round. Let me exit this instance, grab the internal IP address of the instance in the US, and SSH to the instance in Europe.
We're also going to ping the internal IP address here.
We can see that works. We could even now try to ping the external IP address. So that's 34, in my case, 671818, and that works as well. The reason that I'm able to ping the external is because I have firewall rule that allows ICMP externally. I can verify those again. If I click on the network interface details, here I can see all of the firewall rules, and what filters they have, and what protocols, and ports. So this all works fine, and let's assume that this workflow has worked for us but now we have decided that we want to convert the auto mode network that we have to a custom mode network. So let's go ahead and do that. We're going to go to "VPC networks", and we're going to click on "my network", and then we're going to click on "edit", and we're going to change this subnet creation mode from auto to custom, and hit "save".
So now we can navigate back. You can see that this is in progress up here. The mode still says "auto". We could have also flipped that here. Let's wait for that to be refreshed, and now we can see that this subnetwork is now a custom subnet. So let's say that this has worked so far, and now we've realized that we need a couple more networks. There's a network diagram in the lab that has two other networks, as well as some instances and everything. So let's go ahead and create those. So now we're going to go to "create a VPC network". We're going to create the management network, and rather than starting with automatic and converting, we're just going to start with the custom net. For that we have to define each of these subnets. The minimum information we need to provide is a name, the region, so let's select "us-central1", and then the IP address range,
and then can click "done". Now I can add, if I wanted to, another subnet. But the other thing that's very interesting about this is, I'm creating this right now through the GCP console but you can also create networks, as well as subnets, from the command line using Cloud Shell. If I click down here on command line, I'm actually provided with the commands to do that. The first one just creates the network itself. You don't have to use the project flag in here. So we could just say G Cloud compute, networks create, the name of the network, and the fact that this subnet is a custom mode. Similarly then, we create these subnets which is "networks subnets create" the name of the subnets, add the subnet itself, the name of the network, the region, and the range. So again, that's the sort of minimal information. Let's just click close and "create". We'll create the other one from the command line. So it's creating that network, and in parallel I can go and now activate Cloud Shell by clicking up here in the right corner. Yes, I want to start using Cloud Shell. I'm just going to make this a little bit bigger, and once this is up, we're going to use those commands that we just saw to create first a network, and this is going to be the privatenet, which is also of the mode custom. Once we have that, we're going to create two subnets within that network.
So you can see in the console that the other network was created. Privatenet, is being created right now here, and once that is ready we can add the two subnets to that.
So there we go. There is the subnet. It's also telling us this is a new network. You don't have any firewall rules, here are some commands if you want to create some firewall rules. We'll do that in a second. Let's just create these subnets in here. So first we're going to create one in the US, and then we're also going to create one in Europe. If you wanted to speed this up you could actually launch another Cloud Shell session. Now that the network is up, you could create these subnets in parallel. But we're just going to wait for this to complete and then we'll paste that command in there. You can monitor all of this in a console. If we click "refresh," there we see it. It's also completed. It just returns, I've done exactly what you told me. Let's create the other one. It didn't copy the command correctly. There we go.This is now in Europe, specifically Europe-west1. Refresh. You see that's already being created there. So we can definitely display all of those in the GCP console. If you click the button over here in Cloud Shell you can actually open this in a new window. This actually opens it in a new tab, that way you preserve your real estate. You can keep focusing on the console, as well as focusing on Cloud Shell. So let me actually create some real estate by just clearing this, and then paste the command to list all the networks with just G Cloud compute networks list. So we can see them, three networks. They're all Custom. We can dig deeper into this by also listing the subnetworks, and using the "sort-by" command to sort these by network. So now we'll see my network has a lot of subnets because it used to be in auto mode. Then, I mentioned that we want subnet and for permanet. We've two subnets.
So now we're going to create some firewall rules. So let's click on "Firewall rules" up here. You can see the ones that are already there. Click create "Firewall rule". We'll repeat the same process we did earlier. We'll first create this using the console, and then we'll repeat the firewalls for a different network using Cloud Shell. So let me give it a name. Let's make sure I select the right network that the firewall rule applies to. Let's just do all instances. For the IP ranges select all addresses.
I'm allowing, in this case, ICMP, SSH, and RDP. So let me define ICMP, and then 22 for SSH, and 3389 for RDP, and now down here I can click on "command line", and we can see this as one long command. Again, you don't need to define the project flags as gcloud compute, firewalls create. The name of the rule, the fact it's an ingress party, that is also actually default. We could leave that out. Importantly is the name of the network. Action allows default too, and then the rules as well as source ranges. So let's create that in the console, and we'll grab the command from the lab instructions to do the same for either network. So here you can see. We paste that in, and that should now create the other firewall rule for us. We can monitor the firewall rules in the console, as well as in Cloud Shell. So we'll run a command to list all the firewall rules in a second. So they're all created. If we list them, we can see them all here. If I refresh this we can also see them right here.
So now it's time to create some more Instances, and then explore the connectivity. So let's head back to compute engine. I'm going to create instances in these new networks I created. So let me click "create instance". I'm actually going to close Cloud Shell for now. Let me just make it smaller. We're going to provide a name, and "US- Central1-c". Small machine is very fine. Now importantly I need to expand this option down here to select the right network. We've three options right now, and it has actually pre-selected that network. That's because from an order, it's listed up top. That is correct. So let's click "done" and there's again a command. There's a lot of information here that we don't need. You'll see that in a second when we run our command, like the BootDisk. We're selecting a lot of standard options. So let's just hit "create", and let's pull the command from the lab that creates the same in a different network. That's "gcloud compute instance create", the name of the instance, the zone, the machine type, the subnet. That is the bare minimum that we need to provide. So let's run that. You can see the other instance is already created. I can refresh this. See that the other Instances are already coming up too, and once Cloud Shell is updated we can list all the instances. Let's do that here. Can sort them by zone, or we could sort them by network. So we can see in one zone here we have an instance, and then in another zone we have three instances. Keep in mind these Instances are in different networks, and we can display that if we go to columns and check "Network", you can see that these Instances, with exception of the "mynet" these are on the same IVPC network, the others are indifferent. That's going to now go into the connectivity that we're going to explore. We're going to try to again ping IP addresses, both external and internal, and see what works. So let me grab the Management USVM external IP address, and we're going to SSH to the "mynet-us-vm". They are in the same zone, but they're in different networks. So let's see if we can ping the external IP address, and then we'll try the internal.
So external works. That's because we set up the firewall rules for that.
I can also do the same for privatenet. I can plug that IP address in which is 35.188.20.220 That works as well. So you can ping those, even though they are in different networks. Now from an internal perspective I should only be able to ping mynet-uvm which we actually tried earlier already. So let me just hop on the other ones. I'm going to try 10.130.0.2,
and we can see that's not leading to anything. We should be getting a 100 percent packet loss, and then we'll try the same from the other one.
So 172.16.0.2 and we can see that again isn't working either. So even though this Instance is in the same zone as these other instances I'm trying to ping, the fact that they are in a different network does not allow me to ping on the internal IP, unless we set up other mechanisms such as VPC peering or a VPN. That's the end of the lab.

## Common network designs

Let's use what we have learned so far and look at common network designs. Now, common is a fairly relative term, while I could spend all day talking about network designs, I have picked a handful of designs that best relate to this module.
Let's start by looking at availability. If your application needs increased availability, you can place two virtual machines into multiple zones, but within the same subnet work as shown on this slide. Using a single sub-network allows you to to create a file a rule against the sub-network, in this case, 10.2.0.0/16. Therefore, by allocating VMs on a single subnet to separate zones, you get improved availability without additional security complexity. A regional managed instance group contains instances from multiple zones across the same region, which provides increased availability.
Next, let's look at globalization. In the previous design we placed resources in different zones in a single region, which provides isolation for many types of infrastructure, hardware and software failures.
Putting resources in different regions as shown on this slide provides an even higher degree of failure independence. This allows you to design robust systems with resources spread across different failure domains. When using a global load balancer like the HTTP load balancer, you can route traffic to the region that is closest to the user. This can result in better latency for users and lower network traffic costs for your project. We'll explore both managed instance groups and load balancers later in this course series. Now, as a general security best practice, I recommend only assigning internal IP addresses to your VM instances whenever possible. Cloud NAT is Google's managed network address translation service. It lets you provision your application instances without public IP addresses, while also allowing them to access the internet in a controlled and efficient manner. This means your private instances can access the internet for updates, patching, configuration management, and more. In this diagram Cloud NAT enables two private instances to access an update server on the Internet, which is referred to as outbound NAT. However, Cloud NAT does not Implement inbound NAT. In other words, hosts outside your VPC network cannot directly access any of the private instances behind the cloud NAT gateway. This helps you keep your VPC networks isolated and secure. Similarly, you should enable private Google access to allow VM instances that only have internal IP addresses to reach the external IP addresses of Google APIs and services. For example, if your private VM instance needs to access a cloud storage bucket, you need to enable private Google access. You enable private Google access on a subnet by subnet basis. As you can see in this diagram, subnet A has private Google access enabled and subnet B has it disabled. This allows VMA one to access Google APIs and services, even though it has no external IP address.
Private Google access has no effect on instances that have external IP addresses, that's why VMs A2 and B2 can access Google APIs and services. The only VM that can't access those APIs and services is VM B1. This VM has no public IP address and it is in a subnet where Google private access is disabled.

### Lab Review: Implement Private Google Access and Cloud NAT

In this lab, you created an instance with no external IP address and access it using Cloud IAP. You then enable Private Google Access and configured a NAT gateway and verified that vm-internal can access Google APIs and services and other public IP addresses. VM instances without external IP addresses are isolated from external networks. Using Cloud NAT, these instances can access the Internet for updates and patches, and in some cases for bootstrapping. As a managed service, Cloud NAT provides high availability without user management and intervention. Let me walk you through the lab. Now, remember that the GCP user interface can change. So your environment might look slightly different. So the first thing I'm going to do is create the VM instance. After that, we are also going to have to create a VPC network and some firewall rules. So let me go to navigation menu, scroll down to VPC networks.
We're going to create a network and call it privatenet. So I'm going to name it privatenet, keep this subnet creation mode as custom. We're just going to create one subnet in here. We're going to call it privatenet-us. Let's place this in the us-central1 region, as given to us in the instructions. Here we go, and we even have an IP address range for that. Now, we are going to enable Private Google Access later. So you want to keep that off for now. I turned it on by accident. So you can see the effect of it being off. So let me click "Done" and click "Create". Now, I'm going to wait for this network to be created and once it's up and running, we're going to add a firewall rule because we want to allow SSH to the instance that we're going to put on this network. So I can see the network here. A firewall rule is created for networks, so I had to wait for that to be ready. So let me go to firewall rules, create firewall rule, give it a name. Specify that the network is privatenet. Let's just do all instances and sort by IP ranges. Now, rather than just saying, "Hey, you can SSH this instance from anywhere," we are actually going to give it a very specific range. This is because we're using Cloud IAP. So we're going to use a Cloud IAP tunnel, and because of that, we can limit the site range. Now, this is for an SSH connection. So I want to enable TCP port 22,
and then click "Create". While this is creating, I can go ahead and create my Compute Engine instance. So let's go to "Compute Engine", click "Create". We're going to give it the name vm-internal. Now, we need to make sure we choose a region for which we've created a subnet. So us-central1, so us-central1-c. I can keep them machine type as my standard, n1-standard-1, 1virtualCPU, and I'm going to scroll down. The important thing is I need to select the actual VPC networks. Let's go to networking. Networking again, we're going to edit the network interfaces. I want to select the privatenet network. It only has one subnet, and I'm going to set the external IP address to none. Click "Done" and click "Create". So this is a way to create a private instance. Let me close this. That has no external IP address. Now, when the instance comes up, you will see that we won't be able to directly SSH to it because it doesn't have an external IP address. So if we use this, this wouldn't work on us, so instead what we're going to use is, we're going to do an IAP tunnel. For that, we're going to open Cloud Shell. So let me go click "Activate Cloud Shell",
and that popped up in a new window. That can certainly happen sometimes. Looks like there's some A, B testing going on here. So here I have Cloud Shell, doesn't look like it has the correct project set. So let's actually do that. I'm going to set the project and then just grab the project ID from here,
and set this up for the correct project, and there we can see that now. So it's setup, and now what I'm going to do is, I'm going to run the command to SSH from here. I'm going to specify this is through IAP, and then I want to confirm.
For passphrase, we're just going to hit "Enter" and then "Enter" again.
Once this is complete, we should now see that the command prompt has changed to vm-internal. So we're now in vm-internal, it doesn't have an external IP address, but let's confirm that we can't just ping the World Wide Web. This ping command isn't working because vm-internal does not have an external IP address.
So we can wait for this to complete and it's failing. Again, when instances don't have external IP addresses, they can only be reached by other instances on the network, either through a managed VPN gateway or Cloud IAP tunnel, and Cloud IAP enables contexts where access to VMs through SSH and RDP without a bastion host. That would be the other idea or option. We could create a bastion host, but that would still have an external IAP. Then we're just using the bastion host to then connect to this. Instead, we can just use Cloud Shell and IAP. So this isn't working. So what we're going to now is we're going to look into Private Google Access. So currently, VM instance with no external IP address can use Private Google Access to reach external IP addresses of Google APIs and services. But by default, this is disabled. We saw that earlier, we left it as disabled. So let's test the effect of this being disabled. I'm going to go to the navigation menu, and we're going to create a cloud storage bucket. So let's go to "Storage". I'm going to click "Create Bucket". Now, the most difficult piece is you need to have a unique bucket name. You could do that by grabbing the ID of a project. Click Continue, you can leave this as Multi-region, we can leave everything else by default, and just click Create. The important thing is you're going to have to remember that bucket names, so here's the bucket. So I'm going to do now, is I'm going to go back to Cloud Shell. Importantly, I'm still in my VM Instance here. So I want to change that, so let me exit out of here. So now I'm back in Cloud Shell and then I'm going to run a command to copy an image from a public bucket to my bucket, but I need to specify what my bucket is. So I can take the name of the bucket and add that here to copy this image, so that worked. We can go in here and refresh to verify that we now have an image in here. You can actually click on this image and this just shows you how Private Google Access is implemented pending if it's on or off for a network. We're going to explore that a little bit more.
So now what we're going to do is, we're going to now try to copy this image, first from Cloud Shell. Well, Cloud Shell has an external IP address, so that is going to work, run that.
I need to actually click Enter. Obviously, I didn't specify my bucket, that is on me. So I need to change my bucket, so typical error that you might see. Let me grab the name of the bucket, placed it in there. Let's try that again, okay, that works. We even use Cloud Shell to move this image anyway, so we're able to access Cloud Storage currently through Cloud Shell. Let's go back to our VM internal [inaudible]. So we use the same command use earlier to SSH through a IAP tunnel. Here, I can see the command prompt changed. Now, I'm just going to copy the same command here to copy this image, so I don't have to change the bucket name a couple times, and we're going to run that. We should see that this does not work, because currently VM internal can only send traffic within the VPC networks because again, Private Google Access is disabled. So with two options, we can wait for this to fail and give us an error or we can use Control C to just stop the request. So let's actually just stopped this. What we do now is I'm going to able Private Google Access. So let's go back to the Cloud Console, the Navigation menu and I'm going to navigate to my VPC network, specifically privatenet.
Private Google Access is enabled at the subnet level. So I'm going to go directly to the subnet, click the Edit icon, scroll down and able Private Google Access or set it to on, click Save. I'm going to wait for this to update and then I'm going to come back to my instance, my SSH lessons through Cloud Shell and just try to run the command again. So it looks like it's all set, you can also see that here. Going back to my SSH window, run that command again, and now it works. So that's how easy it is to enable Private Google Access. So now in this last task of the lab, we're going to configure a Cloud NAT gateway. Now although our Instance here, VM internal can now access certain Google APIs and services without an external IP address, the instance cannot access the Internet for updates and patches. So for that, we're going to configure our Cloud NAT gateway, but again, we're going to try this behavior first without the NAT gateway and then we're going to enable it. So what we're going to do is I'm going to exit here to just get to my Cloud Shell Instance. There we go, you can see the command prompt changed to Cloud Shell. I'm just going to run sudo apt-get update, and that should obviously work for my Cloud Shell instance because it has an external IP address. So we can see it's getting all these packages and that is working just fine.
So now that's complete, we're going to use the SSH command again using the IP tunnel to get to VM internal, there we can see this change. Now we're going to run the same command here. You might say, "Well, hold on." It's actually able to get some of these packages. Yes, that's because we've enabled Private Google Access, so it's able to get those within Google. Once it's trying to get something else here, it's failing. So we can just stop that, this is not going to happen. Now we're going to go ahead and configure Cloud NAT gateway and then try to run that command again. So let's go to the Cloud Console and under the Navigation menu, we're going to go to Network services and Cloud NAT. We're going to go click Get started, just give this a name called nat-config. It's just a name that we have in the lab instructions. You really want to follow these lab instructions because any of our labs that are scored, we'll use names that we're defining in the lab instructions. So important distance to be on privatenet, Region is us-central1.
For Cloud Router, we currently don't have one, so we're going to go create one. This is super simple, you just give it a name and click Create. Now, there's also a NAT mapping section and this allows you to choose the subnets to map to the NAT gateway, so you could manually assign static IP addresses that should be used when performing that. But in this case, we're not going to go that and get that fancy, we're just going to click Create. We're going to wait now for the gateway status to change to running. So we can see that the status changed to running, it actually only took a couple seconds. Now, even though this is running, it may actually take up to three minutes for the NAT configuration to propagate all the way to the VM. So you want to wait at least a minute before trying to access the Internet again. What I mean by that is in our SSH session that we currently still have to VM internal, we're going to run the command again. I want to make sure it works this time. So I could actually just try it right now and see if it's ready or not. If I do, you see it's still failing at the step. So let me hit Control C and let's get a couple more minutes and then try to run the command again. So we've waited a couple minutes, let's try to run the command one more time and now we can see that's working. It's getting all the packages and with that we can confirm that Cloud NAT decline a gateway is not working. Now, couple of things to remember, the Cloud NAT gateway implements outbound net, but not inbound net. In other words, what that means is that hosts outside of your VPC network, can only respond to connections initiated by your instances. They cannot initiate their own. So new connections to your instances via the net, so keep that in mind. The other thing is in this lab we used IAP, and IAP uses your existing project roles and permissions when you connect to VM instances. So by default, instance owners, which your instance owner since you created this instance. They're the only ones that have the IAP secure tunnel user role. If you want to allow other users to connect to access using VM, using IP tunneling, you need to grant them those roles. You can actually do that directly through the Navigation menu and go to Cloud IP, and just give people those roles. That's the end of the lab.
## Creating virtual machines

In this module, we cover virtual machine instances or VMs. VMs are the most common infrastructure component and then GCP there provided by Compute Engine. A VM is similar but not identical to a hardware computer. VMs consists of a virtual CPU, some amount of memory, disk storage, and an IP address. Compute Engine is GCP service to create VMS. It is very flexible and offers many options including some that can't exist in physical hardware. For example, a micro VM shares a CPU with other virtual machine, so you can get a VM with less capacity at a lower cost. Another example of a function that can't exist in hardware is that some VMs offer a burst capability meaning that the virtual CPU will run a buffets rated capacity for a brief period using the available shared physical CPU. The main VM options are CPU, memory, discs, and networking. Now, this is going to be a very robust module. There's a lot of detail to cover here with how virtual machine work on GCP. First, we'll start with the basics of Compute Engine followed by a quick little lab to get you more familiar with creating virtual machine. Then, we'll look at the different CPU and memory options that enable you to create different configurations. Next, we'll look at images and the different disk options available with Compute Engine. After that, we will discuss very common Compute Engine actions that you might encounter in your day-to-day job. This will be followed by an in-depth Lab that explores many of the features and services covered in this module. Let's get started with an overview of Compute Engine.

### Compute Engine

As I mentioned in the introduction to the course, there is a spectrum of different options in GCP for compute and processing. We will focus on the traditional virtual machine instances. Now the difference is Compute Engine gives you the utmost inflexibility. Run whatever language you want, it's your virtual machine. This is purely an Infrastructure as a Service or IaaS model. You have a VM and an operating system and you can choose how to manage it and how to handle aspects such as autoscaling, where you'll configure the rules about adding more virtual machines in specific situations. Autoscaling will be covered in a later course of this series. The primary work case of Compute Engine is any general workload, especially an enterprise application that was designed to run on a server infrastructure. This makes Compute Engine very portable and easy to run in the Cloud. Other services like Google Kubernetes Engine, which consist of containers workloads may not be as easily transferable as what you're used to find On-premises. So what it is Compute Engine? As it's heard, it's physical servers that you're used to running inside the GCP environment with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You chose the type of disk you want, what do you want to just use standard hard drives, SSDs, local SSDs, or a mix. You can even configure the networking interfaces and run a combination of Linux and Windows machines. Several different features will be covered throughout this module such as machine rightsizing, startup scripts, metadata, availability policies, and pressing, and usage discounts. Let's start by looking at the compute options. Compute Engine provides several different machine types that we'll discuss later in this module. If those machines don't meet your needs, you can also customize your own machine. Your choice of CPU will affect your network throughput. Specifically, your network will scale at two gigabits per second for each CPU core, except for instances with two and four virtual CPUs which receive up to 10 gigabits per second of bandwidth. As of this recording, there's a theoretical maximum throughput of 32 gigabits per second for an instance with 16 or more CPUs, and 100 gigabits per second maximum throughput for specific instances that have T4 of V100 GPUs attached. When you're migrating from an on-premises setup, you're used to physical cores which have hyper-threading. On Compute Engine, each virtual CPU or vCPU is implemented as a single hardware hyper-thread on one of the available CPU platforms. For an up-to-date list of all the available CPU platforms, refer to the links section of this video. After you pick your compute options, you want to choose your disk. You have three options, standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives or HDDs, or flash memory solid state drives SSDs. Both of these options provide the same amount of capacity in terms of disk size when choosing a persistent disk. Therefore, the question really is about performance versus cost because there is a different pricing structure. Basically, SSDs are designed to give you a higher number of IOPS per dollar versus standard disks, which will give you a higher amount of capacity for your dollar. Local SSDs have even higher throughput and lower latency than SSD persistent disks because they're attached to the physical hardware. However, the data that you store on local SSDs persists only until you stop or delete the instance. Typically, a local SSD is used as a swap disk just like you would do if you want to create a RAM disc. But if you need more capacity, you can store those on a local SSD. You can create instances with up to eight separate 375 gigabytes local SSD partitions for total of three terabytes of local SSD space for each instance. Standard and non-local SSD disks can be sized up to 64 terabytes for each instance. The performance of these disks scales with each gigabyte of space allocated. As for networking, we've already seen network and features applied to Compute Engine in the previous modules lab. We looked at the different types of networks and created firewall rules using IP addresses and network tags. You'll also notice that you can do regional HTTPS load balancing and network load balancing. This doesn't require any pre-warming because a load balancer isn't a hardware device that needs to analyze your traffic. A load balancer is essentially a set of traffic engineering rules that are coming into the Google network. VPC is applying the rules destined for your IP address subnet range. We'll learn more about load balancers in a later course of the architecting with Google Compute Engine series.

### Demo: Create a VM

Let me give you a quick walk through of the VM instance creation process, and point out CPU, storage, and network options in the GCP Console. So here I am already on the Compute Engine instance page. You can get to here by going to the navigation menu and then clicking on "Compute Engine". As we use this in the course a lot, you might actually want to pin this sometimes so that you can get to it more easily. Then within there I have gone into VM instances. So I just want to again show you some of the options that are available when creating instance. To get started I'm going to click on "Create". The first thing I want to choose is a name, so you have that right up here. Then maybe more importantly is actually where you want the instance to be located. So you have an option of all the different available regions. It has the name of the regions as well as the closest city as to where that region is located. Then within the regions you have different zones that you can choose from. You also see on the right-hand side that there is a cost associated with the current configuration, net cost is going to change as we change the configuration. So for example, if I instead of creating an instance in US Central one, I create one maybe in Europe West One. You will see that the cost is slightly adjusted. So I can try that a couple different ways by choosing different locations. You should see that the cost changes depending on the region that we choose. Now it goes further if I then choose the machine type. There are different types, we'll go into all of those. But if I go in here, the different types explain to me what they provide. This standard n1 standard one provides one virtual CPU with 3.75 gigabytes of memory. If I change to a machine with more CPU and more memory, we'll see that the cost is adjusted. You can also go into details here. It actually spells it out for you that there's a cost for the CPU and memory, but there's also costs for the persistent disk. We haven't configured that yet, but this is the default value and if we configure that, it's going to ingest a cost. There's also sustained use discount would go into that as well, but essentially all of that is what ultimately gets you to this total monthly cost. It's also broken down in an hourly cost here, and we'll talk more about pricing later within the module. So again I can choose different machine types, maybe we want a larger machine type that's going to be more expensive. Maybe I just need a shared core. So something really, a micro machine or a small machine and that can really drive the cost down a lot. So let me go back to the default. The other thing to think about in terms of your region and zone is not just the cost, but really you want to create your instances that are close to your users. Maybe you want to have them spread out across different regions for habilability. You might be having restrictions for data locality meaning that your data has to be in a specific region. So these are all the different things that you want to consider when choosing the region and zone. Now if I scroll further down, one of the next pix options is the boot disk. So we can see here that currently it has a 10 gigabytes standard persistent disk. I can change that. I can change the image itself, but I can also change the boot disk type. Now the boot disk needs to be a persistent disk. We have the standard persistent disk Think of an HTTP, and we have the SSD. You can see that we can define the size here, and you can see that both of them have the same exact maximum size. So if I make this larger, let's say 1,000, then we're going to see that the cost now is adjusted to that disk size. So it can go back, that's very large. Maybe I'm just okay with 10 gigabytes as the boot disk. You can also add more disks. So if I scroll down and go to management security disk networking, I can go to disks here. So here I can choose the type of encryption for the disk. I have Google managed key, Customer managed key, Customer supplied key. Then I can add more disks. So if I add a new disk here then under type I could also choose a local SSD. Disks come in predefined sizes, depending on how many you choose your performance as you can see down here, is going to get adjusted. There is a limit. So at some point the more disk you choose, you're going to hit a limit into your performance, and same if I choose an SSD disk and change the size here, you'll see that also there's a limit but it's also adjusted if I scale as you are changing the IOPS as well as the sustained throughput limit. Now another important thing is obviously networking. So if I click on here, you want to choose the network interface. We already went into this a little bit in the previous module in terms of near choosing your primary internal IP, choosing if you want an external IP or not. So those are all of the different options that you can get there. Now what's really cool is this is all using the GCP Console, but down the road you might say well, I want to create these instances quickly and I want to use a command line. Well, this user interface gives you the command line options. So it's spelling out exactly all the different options you have chosen, how you would recreate that using GCloud. So this can help you get started using the command line and make you more comfortable using that command line. So let me just go ahead and create this instance. Once we create it we have these different columns that are listed here, there are more columns that we can choose from. For example, when you created it, what the machine type is, what network this is a part off, if you had labels or other things, so lots of different options you can list here. So for example, I can just hear when the machine was created, the type as well as the network it is a part off. That's how easy it is to configure the location, CPU, memory, storage, and network interface for a VM instance using the GCP Console. Let's get back to the slides to go over VM axis and lifecycle.

### VM access and lifecycle

For accessing a VM, the creator of an instance has four root privileges on that instance. On a Linux instance, the creator has SSH capability and can use the GCP Console to grant SSH capability to other users. On a Windows instance, the creator can use the GCP Console to generate a username and password. After that, anyone who knows the username and password can connect to the instance using a remote desktop protocol or RDP client. I listed the required firewall rules for both SSH and RDP here but you don't need to define these if you're using the default network that we covered in the previous module. For more information on SSH key management and creating passwords for Windows instances, refer to the links section of this video. The life cycle of a VM is represented by different statuses. I will cover this life cycle on a high level but I recommend returning to this diagram as a reference. When you define all the properties of an instance, and click "Create" the instance enters the provisioning state. Here the resources such as CPU, memory, and disk are being reserved for the instance but the instance itself isn't running yet. Next, the instant moves to the staging state where resources have been acquired and the instance is prepared for launch. Specifically in this state Compute Engine is adding IP addresses, booting up the system image, and booting up the system. After the instance starts running, it will go through pre-configured startup scripts and enable SSH or RDP access. Now, you can do several things while your instance is running. For example, you can live migrate your virtual machine to another host in the same zone instead of requiring your instance to be rebooted. This allows GCP to perform maintenance that is integral to keeping the infrastructure protected and reliable without interrupting any of your VMs. While you're instance is running, you can also move your VM to a different zone. Take a snapshot of the VMs persistent disk, export the system image or reconfigure metadata. We will explore some of these tasks in later labs. Some actions require you to stop your virtual machine. For example, if you want to upgrade your machine by adding more CPU. When the instance enters this state, it will go through pre-configured shutdown scripts and end in the terminated state. From this state, you can choose to either restart instance which would bring it back to its provision state or delete it. You also have the option to reset a VM which is similar to pressing the reset button on your computer. This action wipes the memory content of the machine and resets the virtual machine to its initial state. The instance remains in the running state throughout the reset. There are different ways you can change a VM state from running. Some methods involve the GCP Console and the GCloud command while others are performed from the OS such as for a reboot and shut down. It's important to know that if you're restarting, rebooting, stopping, or even deleting an instance, the shutdown process will take about 90 seconds. For a preemptible VM, if the instance is not stopped after 30 seconds, Compute Engine sends an ACPI G3 mechanical off signal to the operating system. Remember that when writing shutdown scripts for preemptible VMs. As I mentioned previously, Compute Engine can live migrate your virtual machine to another host due to a maintenance event to prevent your applications from experiencing disruptions. A VMs availability policy determines how they instance behaves in such an event. The default maintenance behavior for instances is to live migrate, but you can change the behavior to terminate your instance during maintenance events instead. If your VM is terminated due to a crash or other maintenance event, your instance automatically restarts by default but this can also be changed. These availability policies can be configured both during the instance creation and while an instance is running by configuring the automatic restart and on host maintenance options. For more information on live migration, refer to the link section of this video. When a VM is terminated, you do not pay for memory and CPU resources. However, you are charged for any attached disks and reserved IP addresses. In the terminated state, you can perform any of the actions listed here such as changing the machine type, but you cannot change the image of a stopped VM. Also, not all of the actions listed here require you to stop a virtual machine. For example, VM availability policies can be changed while the VM is running as discussed previously.

### Lab Review: Creating virtual machines

In this lab, you created several Virtual Machine instances of different types with different characteristics. Specifically, you created a small utility VM for administration purposes, a windows VM, and accustomed Linux VM. You also acts as both the Windows and Linux VM and deleted all your creative VMs. In general, start with a smaller VM when you're prototyping solutions to keep the cost down. When you're ready for production, trade up to larger VMs based on capacity. If you building and redundancy for availability, remember to allocate excess capacity to meet performance requirements. Finally, consider using custom VMs when your applications requirements fit between the features of the standard types. You can stay for a lab walk through but remember that GCP user interface can change, so your environment might look slightly different. So in the GCP console, I'm going to navigate to Compute engine and then VM instances, and in here we're going to click "Create". Now, we can define a name there's a small question mark here and if you hover over it can tell you a little bit more about some of the restrictions you have in regards to creating a name, choosing a name that is, and I'm just going to call this my utilityVM. We're going to go over some of the options that actually went over a little bit in the demo, but we obviously can choose region and zones. So let's change the zone to what the lab is instructing, which is 1-C, and then for the machine type we have a lot of different options to choose from. We can see that the cost changes if I scale up to a machine with four virtual CPUs versus a machine that's just maybe a micro, which is a shared core machine. So the cost can change quite drastically. So let's just leave all the remaining settings and click "Create", and once the machine is up and running, we're going to explore the different VM details that we have. So we're going to go into the VM Instances page, and look at things like the CPU platform, the availability policies and so on. So let me do that, let me click on "Utility VM" because it's now in a running state. I'm going to look for a CPU platform, you can see that right here and if I click "Edit", you'll see that I actually am unable to modify that. So that's because I can't do that while the instances is running. There are other things I could do, I could change the firewall rules, I can add network tags. So certain things are available to change while and instances is running. In some cases, you have to stop the instance to change some of the properties. In other cases, you cannot actually even change it unless you delete it. One of those is for example the network interfaces, if you had multiple network interfaces, you'd have to recreate your instance. The good thing is you could keep your boot disk and just reattach that boot disk later on. Now, I can also go and look at the availability policies, just scroll down to some what the enhanced maintenance is. By default, it's set to migrate the VM instance, and that's recommended but you could set this to terminate the instance. It's also going to automatically restart that instance so you could configure that as well. All right, so this is just a little bit exploring the different options, I'm going to go click "Cancel". What we're going to now is explore some of the VM logs. So I'm looking at the detail page here. We want to get a little bit more information about the monitoring options that are available. We can click "Monitoring" here, and we'll get more information about the CPU. This instances barely runs, we don't have much data yet. We get information about the network bytes and packets, disk I/O. We can also, if we go back to details, look at stackdriver logging. So this is now a different user interface and here we now have individual logs that we can explore. We can view options here, we could expand all of these and dig into all of these different logs that are in here and even within there, expand each of the logs to get more information. So this uses stackdriver logging, we'll cover this feature a little bit but more in a later course in the course series if you're interested to learn more about both the logging piece that we just looked at as well as the monitoring. So let's go to Test 2, we're now going to create a windows virtual machine. So I'm going to go back through the navigation menu Compute engine to VM instances, and I'm not going to create a another instance.
So I'm going to define a name, and this is just going to call it Windows VM, and we're going to choose a different region and zone this time. Why don't we put this into Europe-West2, and specifically to zone 2A. Let's pick a larger machine. Let's pick one that has two virtual CPUs and 7.5 gigabytes of memory. We can even go ahead now and changed the boot disk because by default, this would be a Linux machine, so if we want to change this because we want to create a windows machine. Specifically, the lab is instructing me to look for the Windows Server 2016 Datacenter Core image. It's first scroll down, I can see that image right here, can change the boot disk. Maybe I want some higher IOPS, I can choose an SSD, and I can even make this larger and click "Select". All of that again is going to affect obviously the cost. I have the cost of the machine, I have the cost of the disc, but the new thing I have now also is the image, I've chosen zupimages which means there is a cost associated with using that image, but it's built all together for you. So you can see that cost broken up right here. Now, the other thing we're going to do is we're going to allow a specific traffic, HTTP and HTTPS traffic. This just creates a network tag for us and then creates filer roles on the network tag so that we can enable traffic on those ports for the TCP protocol. So let's hit "Create" and create this instance. One thing we'll notice when the instance comes up is that under the connect column [inaudible] now seeing an SSH button which is we would have for a Linux machine. We should now see a RDP, which is for the Remote Desktop Protocol. So that's how you would access a Windows machine. Now, the important thing is there you obviously want to configure your username and password so that only authorized users access that machine. So here you can see the RDP button now. What we're going to do now is we're going to click onto the machine and set the Windows password. You can actually also do this by clicking "Down here" set windows password there as well. So actually, let's just do it that way. So you have a username here. It's taking the username that I have for my lab account. So this is the username right now. So I can set that and then it's going to provide me with a password. So there we go. So I can now copy that password and if I use an RDP connection, I can then get into that. This is a little bit outside of the scope for this lab, but if you want to and have an RDP client, you can actually install one through Chrome, through an extension. You could access that instance that way and then configure it and do anything else you wanted to in this Windows Virtual Machine. So let me go ahead and close that, and I'm going to move on to a Task 3, now which is to create a custom Virtual Machine. So I'm going to go back to Create Instance, and to find a name, let's just call it my custom-VM. I'll follow the lab instructions here for setting the region and zone which is US-West1-B. Now, rather than choosing a specific machine type, I can go in here and just select Custom as the machine type and then define the exact numbers of cores memory. So let's say, my specification I want six virtual CPU, and you can see how the scales by the way, there are only certain options. You can choose it goes all the way to 96. So let me choose six here. It's going to scale that memory automatically for us. It gives us a range now depending on that CPU, there's an option to extend the memory so you could get more than 39 and see all the way to 624. This is a separate option, we'll talk more about this in the slides. So let me choose 32, and rather than scrolling here I could also just type the value in and that's also going to adjust the cost now. Sometimes, it's important to note that your custom machine may be between two machine types are actually already provided. A custom machine is generally going to be slightly more expensive. So if you have a standard machine that's very close to the custom machine, it's definitely something you would want to consider. Once the machine runs more than 24 hours, you'll get right sides recommendations. So It'll tell you if the machine is too small or too large and make recommendations based on that. So let's go ahead and create that.
Once it's up and running, we're going to SSH to the machine. We're going to run some commands on that machine, and that's actually going to wrap up the Lab for us.
Now, with any new project, you get this column here on the right-hand side to help you get started because we're using Qwiklabs generated projects, they're always going to be new products. So you'll see this throughout the training. You can certainly leveraged this if you want but I'm going to collapse that. So VM is up and running, let me SSH to it.
Then we're going to run the free command to see information about any unused and used memory and swap space. So let me type free. So we can see that here and that lines up with the memory selections that we made in the machine. I can also see I get more information or details about the RAM installed. So here we get more information about that as well. I can verify the number of processors. So that should have been six, and yep, and prox is sixth, great. We can see details about the CPU itself. So here we get information about the architecture, the byte order, which model exactly, so you can get all this information about any VM that you create. You can also get more information about this in the documentation depending on which region and zone you choose. You'll have different architectures and different models available to choose from.
So that's all we wanted to show you here with this lab. We went ahead and created that Virtual Machine, the Utility VM, we created a Windows VM, and then we created a custom Virtual Machine and verified that whatever custom settings we applied were actually used to create the machine by running commands within that machine.


## Working with the virtual machines


### Compute options

Now that you have completed the lab, let's dive deeper into the compute options that are available to you in GCP, by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the GCP console as you did in the previous lab, the Cloud Shell command-line, or the RESTful API. If you'd like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the GCP Console, and then ask Compute Engine for the equivalent REST request or command-line as I showed you in my demo earlier. This way you avoid any typos and get drop-down lists of all the available CPU and memory options. Speaking of CPU memory options, let's look at all the different machine types that are currently available. A machine type specifies a particular collection of virtual hardware resources available to a VM instance, including the system memory size, vCPU count, and maximum persistent disk capability. GCP offers several machine types that can be grouped into two categories. Predefined machine types. These have fixed collection of resources, are managed by Compute Engine, and are available in multiple different classes. Each class has a predefined ratio of gigabytes of memory per Virtual CPU. These are the standard machine types, high memory, high CPU, memory optimized, Compute optimize, and shared-core machine types. There also the custom machine types. These lets you specify the number of virtual CPUs, and the amount of memory for your instance. Let's explore each of these Machine types. But remember that these Machine types and the available options can change. Standard machine types are suitable for tasks that have a balance of CPU and memory needs. Standard machine types have 3.75 gigabytes of memory per virtual CPU. The virtual CPU configurations come in different intervals from 1vCPU all the way to 96 vCPUs as shown on this table. Each of these machines supports a maximum of 128 persistent disks with a total persistent disk size of 64 terabytes, which is also the case for the high memory, high CPU, memory optimized, and compute optimized machine types. High memory machine types are ideal for tasks that require more memory relative to vCPUs. High memory machine types have 6.5 gigabytes of system memory per vCPU. Similar to the stent machine types, the vCPU configurations come in different intervals from 2vCPUs all the way to 96vCPUs, as shown on this table. High CPU machine types are ideal for tasks that require more vCPUs relative to memory. High CPU machine types have 0.9 gigabytes of memory per vCPU. Memory optimized machine types are ideal for tasks that require intensive use of memory with higher memory to vCPU ratios than high memory machine types. These machine types are perfectly suited for in-memory databases and in-memory analytics such as SAP HANA and business warehouse workloads, genomic analysis, and SQL Analysis Services. Memory optimized machine types have more than 14 gigabytes of memory per vCPU. These Machines come in four configurations as shown in this table, with only the n1-megamem-96 supporting a local SSD, as of this recording. Compute optimized machine types are ideal for compute intensive workloads. These machine types are for the highest performance per core on Compute Engine. Built on the latest generation Intel scalable processors, the casket lake, C2 machine types offer up to 3.8 gigahertz sustained all-core turbo, and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. C2 machine types offer much more computing power, run on a newer platform, and are generally more robust for compute intensive workloads than the n1 high CPU machine types. Shared-core machine types provide one virtual CPU that is allowed to run for a portion of the time on a single hardware hyper-thread on the host CPU running your instance. Shared-core instances can be more cost effective for running small non resource intensive applications than other machine types. There are only two shared-core machine types to choose from, they're the f1-micro and the g1-small. The f1-micro machine types offer bursting capabilities that allow instances to use additional physical CPU for short periods of time. Bursting happens automatically when your instance requires more physical CPU than you originally allocated. During these spikes, your instance will opportunistically take advantage of available physical CPU in bursts. Note that bursts are not permanent and are only possible periodically. For up-to-date information about all of these machine types, see the link section of this video. If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios; when you have workloads that are not a good fit for the predefined machine types that are available to you, or when you have workloads that require more processing power or more memory but you don't need all of the upgrades that are provided by the next larger predefined machine type. It cost slightly more to use a custom machine type than equivalent predefined machine type. There are still some limitations in the amount of memory and vCPUs you can select. Only machine types with one virtual CPU or an even number of virtual CPUs can be created. Memory must be between 0.9 gigabytes and 6.5 gigabytes per virtual CPU by default. The total memory of the instance must be a multiple of 256 megabytes. By default, a custom machine can have up to 6.5 gigabytes of memory per virtual CPU. However, this might not be enough memory for your workload. So at an additional cost, you can get more memory per virtual CPU beyond the 6.5 gigabytes limit. This is referred to as extended memory, and you can learn more about this in the links section of this video. The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. This map shows the current and planned GCP regions and the number of zones. For up-to-date information on the available regions and zones, see the documentation linked for this video. Each zone supports a combination of Ivy Bridge, Sandy Bridge, Haswell, Broadwell, and Skylake platforms. When you create an instance in the zone, your instance will use the default processes supported in that zone. For example, if you create an instance in the US-central1 a zone, your instance will use the Sandy Bridge processor.


### Compute pricing

GCP offers a variety of different options to keep the prices low for compute engine resources. All of these vCPUs, GPUs and gigabyte of memory are charged a minimum of one minute. For example if you've run your virtual machine for 30 seconds, you will bill it for one minute of usage. After one minute, instances are charged in one second increments. Compute Engine uses a resource-based pricing model where each virtual CPU and each gigabyte of memory on Compute Engine is built separately rather than as part of a single machine type. You still create instances using predefined machine types, but your bill reports them as individual vCPUs and memory used. There are several discounts available but the disk on types cannot be combined. There are research-based pricing which allows Compute Engine to apply sustained use discounts to all of your predefined machine types usage in a region collectively rather than to individual machine types. If you're workload is stable and predictable, you can purchase a specific amount of vCPU and memory for a discount off of normal prices in return for committing to a usage term of one or three years. The discount is up to 57 percent for most machine types or custom machine types. The discount is up to 70 percent for memory optimized machine types. A preemptible VM is instance that you can create and run at much lower price than normal instances. However, Compute Engine might terminate or preempt these instances if it requires access to those resources for other tasks. Preemptible instances are access Compute Engine capacity, so their availability varies with usage. The ability to customize the amount of memory in CPU through custom machine types allows for further pricing customization. Speaking of sizing your Machine, Compute Engine provides VM sizing recommendations to help you optimize the resource use of your virtual machine instances. When you create a new instance, recommendations for the new instance will appear 24 hours after the instance has been created. Compute Engine also has Free usage limits, for the exact terms, please refer to the links section of this video. Sustained use discounts are automatic discounts that you get for running specific Compute Engine resources, be it CPUs, memory, and GPU devices for a significant portion of the billing month. For example when you run one of these resources for more than 25 percent of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. The discount increases with usage, and you can get up to 30 percent net discount for instances that run the entire month. This table shown on this slide describes the discount you get at each usage level of a VM instance. To take advantage of the full 30 percent discount, create your VM instances on the first day of the month because discounts reset at the beginning of each month. The graph on this slide demonstrates how your effective discount increases with use. For example, if you use a virtual machine for 50 percent of the month, you get an effective discount of 10 percent. If you use it for 75 percent of the month, you get an effective discount of 20 percent, and if you use it for a 100 percent of the month, you get an effective discount of 30 percent. You can also use the GCP pricing calculator to estimate your sustained use discount for any arbitrary workload. For the calculator, see the links section of this video. Compute Engine calculates sustained use discounts based on vCPU and memory usage across each region, and separately for each of the following categories, pre-defined machine types and custom machine types. Let's go through an example where you have two instances that are in the same region but have different machine types, and run at different times of the month. Compute Engine breaks down the number of vCPUs and amount of memory used across all instances that use predefined machine types, and combines the resources to qualify for the largest sustained usage discounts possible. As shown on this slide, you run the following two instances in the US Central one region during a month. For the first half of the month, you run an n1-standard-4 instance with four virtual CPUs, and 15 gigabytes of memory. For the second half of the month, you run a larger n1-standard-16 instance with 16 virtual CPUs, and 60 gigabytes of memory. In this scenario, Compute Engine reorganizes these machine types into individual vCPUs and memory resources, it combines their usage to create the following resources as shown on the bottom. Four virtual CPUs and 15 gigabytes of memory for a full month, and then 12 virtual CPUs and 45 gigabytes of memory for half of the month.

Sustained use: more utilization per month => more discount

### Special compute configurations

As I mentioned earlier a preemptible VM is an instance that you can create and run at much lower prices than normal instances. See whether you can make your application function completely on preemptible VMs, because an 80 percent discount is a significant investment in your application. Now, just to reiterate, these VMs might be preempted at any time, and there is no charge if that happens within the first 10 minutes. Also, preemptible VMs are only going to live for up to 24 hours, and you only get a 30-second notification before the machine is preempted. It's also worth noting that there are no life migrations, no automatic resorts in preemptible VMs. But something that we will highlight is that you can actually create monitoring and load balances then can startup new preemptible VMs in case of a failure. In other words, there are external ways to keep restarting preemptible VMs if you need to. One major use case of preemptible VMs is running a batch processing job. If some of those instances terminate during processing, the job slows down but does not completely stop. Therefore preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances. If you have workloads that require physical isolation from other workloads, or virtual machines in order to meet compliance requirements, you want to consider sole-tenant nodes. A sole-tenant node is a physical Compute Engine server that is dedicated to hosting VM instances only for your specific project. Use sole-tenant nodes to keep your instances physically separated from instances in other projects, or to group your instances together in the same host hardware. For example if you have a payment processing workload that needs to be isolated to meet the compliance requirements. The diagram on the left shows a normal host with multiple VM instances from multiple customers. A sole-tenant node as shown on the right, also has multiple VM instances, but they all belong to the same project. As of this recording, the only available node type can accommodate VM instances up to 96 V CPUs and 624 gigabytes of memory. You can also fill the node with multiple smaller VM instances of various sizes including custom machine types and instances with extended memory. Also if you have existing operating system licenses, you can bring them to Compute Engine using sole-tenant nodes while minimizing Physical Core usage with the in-place restart feature. To learn how to create nodes and place your instances on those nodes, see the links section of this video. Another Compute option is to create shielded VMs. Shielded VMs offer verifiable integrity of your VM instances. So you can be confident that you're instances haven't been compromised by boot or kernel level of malware or rootkits. Shielded VMs verifiable integrity is achieved through the use of secure boot, Virtual Trusted Platform Module or VTPM enabled measured boot and integrity monitoring. Shield VMs is the first offering in the shielded Cloud initiative. The shielded Cloud initiative is meant to provide an even more secure foundation for all of GCP by providing verifiable integrity, and offering features like VTPM shielding or ceiling that help prevent data exfiltration. In order to use the shielded VM features, you need to select a shielded image. We'll learn about images in the next section.

Preemptible VMs cost up to 80 percent less than standard ones
  - might be terminated at any time
  - 24 hours maximum
  - 30sec termination warning
  - no live migrate
  - no auto restart
  - CPU quota for region can be split between standard and preemptible VMs
  - typical case is batch processing jobs

Sole-tenant nodes is dedicated hypervisors, can bring own software licenses

Shielded VMs add additional integrity:
  - secure boot
  - virtual trusted platform module (vTPM)
  - integrity monitoring
  

### Images

Next, let's focus on images. When creating a virtual machine, you can choose the boot disk image. This image includes the boot loader, the operating system, the file system structure, any pre-configured software, and any other customizations. You can select either a public or custom image. As you saw in the previous lab, you can choose from both Linux and Windows images. Some of these images are premium images as indicated in parentheses with a p. These images will have per second charges after a one-minute minimum, with the exception of SQL Server images, which are charged per minute after a 10-minute minimum. Premium image prices vary with the machine type. However, these prices are global and do not vary by region or zone. You can also use custom images. For example, you can create and use a custom image by pre installing software that's been authorized for your particular organization. You also have the option of importing images from your on-premises or workstation, or from another cloud provider. This is a no cost service that is as simple as installing an agent. I highly recommend that you look at it. You can also share custom images with anybody in your project or among other projects too.

Boot image consists of:
  - boot loader
  - OS
  - file system structure
  - software 
  - customizations

### Disk options

At this point, you've chosen an operating system, but that operating system is going to be included as part of some kind of disk. So let's look at the disk options. Every single VM comes with a single root persistent disk because you're choosing a base image to have that loaded on. This image is bootable and that you can attach it to VM and boot from it. It's durable and that it can survive, if the VM terminates. To have a boot disks survive a VM deletion, you need to disable the delete boot disk when instance is deleted option in the instances properties. As I discussed earlier, there are different types of disks. Let's explore these in more detail. The first is that we create, is what we call a persistent disk. That means it's going to be attached to the VM through the network interface. Even though it's persistent, it's not physically attached to the machine. The separation of disk and compute, allows a disk to survive if the VM terminates. You can also perform snapshots of these disks which are incremental backups that we'll discuss later. The choice between HDD and SSD disk comes down to cost and performance. To learn more about the disk performance and how it scales with disk size, see the links section of this video. Another cool feature of persistent disks, is that you can dynamically resize them, even while they are running and attached to a VM. You can also attach a disk in read only mode to multiple VMs. This allows you to share static data between multiple instances, which is cheaper than replicating your data to unique disks for individual instances. By default, computer engine encrypts all data at rest. GCP handles and manages this encryption for you, without any additional actions on your part. However, if you wanted to control and manage this encryption yourself, you can either use Cloud key management service, to create and manage key encryption keys, which is known as customer managed encryption keys. Or you can create and manage your own key encryption keys known as customer supplied encryption keys. Now, local SSDs are different from persistent disks and that they're physically attached to the virtual machine. Therefore, these disk are ephemeral, but provide very high IOPS. For up to date numbers, I recommend referring to the documentation. Currently, you can attach up to eight local SSD disks with 375 gigabytes each, resulting in a total of three terabytes. Data on these disks will survive a reset, but not a VM stop or terminate. Because these disks can't be reattached to a different VM. You also have the option of using a RAM disk. You can simply use TM PFS if you want to store data in memory. This will be the fastest type of performance available if you need small data structures. I recommend a high memory virtual machine if you need to take advantage of such features, along with a persistent disk to backup the RAM disk data. In summary, you have several different disk options. Persistent disk can be rebooted and snapshotted, but local SSDs and RAM disks are ephemeral. I recommend choosing a persistent HDD disk when you don't need performance but just need capacity. If you have high-performance needs, start looking at the SSD options. The persistent disk offer data redundancy because the data on each persistent disk is distributed across several physical disks. Local SSDs provide even higher performance but without the data redundancy. Finally, RAM disks are very volatile, but they provide the highest performance. Now, just as there is a limit on how many local SSDs you can attach to VM, there's also a limit on how many persistent disks you can attach to VM. As illustrated in this table, this limit depends on the machine type. For the shared core machine type, you can attach up to 16 disks. For the standard high CPU memory optimized and compute-optimized machine types, you can attach up to 128 disks. So you can create massive amounts of capacity for a single host. Now, remember that little nuance when I told you about how throughput is limited by the number of cores that you have. That throughput also shares the same bandwidth with disk IO. So if you plan on having a large amount of disk IO throughput, you will also compete with any network egress or ingress throughput. So remember that, especially if you'll be increasing the number of drives attached to a virtual machine. There are many differences between a physical hard disk in a computer and a persistent disk, which is essentially a Virtual Network device. First of all, if you remember with normal computer hardware disks, you have to partition them. Essentially, you have a drive and you're carving up a section for the operating system to get its own capacity. If you want to grow it, you have to repartition it and if you want to make changes you might even have to reformat. If you want redundancy, you might create a redundant disk array and if you want encryption, you need to encrypt files before writing them to the disk. With Cloud persistent disks, things are very different because all that management is handled for you on the back-end. You can simply grow disks and resize a file system because disks are Virtual Network devices. Redundancy and snapshots services are built-in and disks are automatically encrypted. You can even use your own keys and that will ensure that no party can get to the data except you.

Persistent SSD disks - high but very random IOPS

Local SSDs are physically attached to VMs
  - highest IOPS, lowest latency
  - 375GBs, up to 8 disks (up to 3TB total)
  - data survives a VM reset, but not stop/terminate instance
  - cannot be reattached to different VMs
  - no snapshots
  - not bootable
  
RAM disk - only for tmpfs

Shared-core machines => up to 16 persistent disks
All other machine types => up to 128 persistent disks

### Common Compute Engine actions

Now that we have covered all the different compute image and disk options, let's look at some common actions that you can perform with Compute Engine. Every VM instance stores its metadata on a metadata server. The metadata server is particularly useful in combination with startup and shutdown scripts because you can use the metadata server to programmatically get unique information about an instance without additional authorization. For example, you can write a startup script that gets the metadata key value pair for an instance's external IP address and use that IP address new script to setup a database. Because the default metadata keys are the same on every instance, you can reuse your script without having to update it for each instance, this helps you create less brittle code for your applications. Storing and retrieving instance metadata is a very common Compute Engine action. I recommend storing these startup and shutdown scripts in Cloud Storage as you will explore in the upcoming lab of this module. Another common action is to move an instance to a new zone. For example, you might do so for geographical reasons or because a zone is being deprecated. If you move your instance within the same region, you can automate the move by using the gcloud compute instances move command. If we move your instance to a different region, you need to manually do so by following the process outlined here. This involves making a snapshot of all persistent disks and creating new disks in the destination zone from that snapshot. Next, you create a new VM in the destination zone and attach the new persistent disks, assign a static IP, and update any references to the VM. Finally, you delete the original VM, its disks and the snapshot. Speaking of snapshots, let's take a closer look at these. Snapshots have many use cases. For example, they can be used to backup critical data into a durable storage solution to meet application, availability, and recovery requirements. These snapshots are stored in Cloud Storage, which is covered later. Snapshots can also be used to micro data between zones. I just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard ECD persistent disk to a SSD persistent disk. Now that I've covered some of these snapshot use cases, let's explore the concept of a disk snapshot. First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images which are used primarily to create instances or configure instance templates, in that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. As we saw with the previous examples, snapshots can be restored to a new persistent disk, allowing for a move to a new zone. To create a persistent disk snapshot, see the link section of this video. Another common Compute Engine action is to resize your persistent disk. The added benefit of increasing storage capacity is to improve I/O performance. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disk and size, you can never shrink them. So keep this in mind.

Move VM between regions:
  - automatic or manual
  - snapshot all persistent disks on source VM
  - create new persistent disks in destination zone restored from snapshots
  - create new VM in destination zone and attach new persistent disks
  - assign static IP
  - update references to VM
  - delete snapshots, original disks and VM

### Lab Review: Working with Virtual Machines

In this lab, you created a customized Virtual Machine instance by installing base software which was a headless Java runtime environment and application software specifically a Minecraft Game Server. You customize the VM by preparing and attaching a high-speed SSD and you've reserved a static external IP address so that the address will remain consistent. Using that IP address, you then verify the availability of the gaming server online. Next, you set up a backup system to backup the service data to a Cloud Storage bucket, and then you tested that backup system. You then automated backups using cron. Finally, you set up maintenance scripts using metadata for graceful startup and shutdown off the server. Many of these techniques including these script automation can be adapted to administration of production servers in any application. You can stay for a lab walk-through. But remember that GCPs user interface can change. So your environment might look slightly different. So here I am in the VM Instances page. Let's go ahead and create our instance. We're going to use the same properties that are provided to us, properties and values in the lab. So I'm going to call this the mc server for our Minecraft Server. We're going to place this in the US Central one, a zone. We're going to modify the access scopes for this. So I'm going to set axis for each API. I'm going to modify for storage that besides just read only, I want read write. This is going to allow the VM instance to write to the Cloud Storage bucket that we're going to create later on. Now we're also going to modify the disk of this instance. So let's expand the option down here, and under disks we're going to add a new disk.
We're going to call this the Minecraft disk,
and we're going to make that an SSD persistent disk. It's going to be blank so no source. Fifty gigabytes is more than enough for what we're trying to do. I'm going to leave the encryption as Google managed key. So let me click done and this is going to create that disk and automatic attach it to the VM. Now under networking, we're also going to add a network tag. This is going to then allow us to locate specific firewall rules, we call that Minecraft Server. On the network interface I'm going to click on the pencil icon here to edit. We are leaving the internal IP as is but for the external IP, we're actually going to create an IP address which means that we are reserving a static IP address. This is going to make sure that these IP address is not ephemeral and doesn't change. So I'll just give it a name and I click reserve and then we're going to click done once that is reserved and from there we're going to create this instance. It's done and then create. Now once the instance is up and running we're going to have to prepare the data disks. So we're not going to create a directory, format and mount the disk. I don't need this tab over here so I can close that. We're going to wait for the instance there it is. So the SSH to the instance. I'm going to start by creating a director that serves as the amount point for the data off the disk. For that I'm just going to use the command that's provided in the lab and then we're going to format the disk itself. So we're just going to wait for that SSH connection to be established. This is allowed because the default network has a default firewall rule for SSH. So let me go ahead and run that, and then we're going to format the disk.
Great. Now we're going to mount it, and this is not going to display any outputs, so don't be surprised about that. There's a checkpoint in the lab. So you can check your progress, worked for me. So I'm going to move on to task three and now install and run the application and the micro server itself runs on top of the Java virtual machine. So we do require the Java Runtime Environment or GRU to run. But we don't need the user interface. So we're just going to install actually headless version and that's going to reduce a lot of the resource usage on that machine which will ensure that the Minecraft Server has enough room to expand its own resource usage if needed. So let me go ahead and start by updating the repository. Then I'm going to install that headless GRU,
and after that I'm going to navigate to the directory where we mounted that persistent disk. Into that we're then going to download the Minecraft jar file.
So we navigate into that under command. You can see it's downloading and the lab manual also provides information on the download page itself. So you can read more about where this comes from. There are also lots of instructions actually in there on how to set this up on a Linux machine. So if you wanted to customize this, I definitely recommend referring to that link. So let's go ahead and initialize the Minecraft Server.
Run that command and it's telling us that this is not going to run unless we agree to the end-user licensing agreement. So we need to do that now. Let me just check my progress. Make sure that the GRU installation and Minecraft server installation worked out and I got a green check in my lab. So let's look at the files that were created to identify where this license agreement is, and there it is. We can see it right there. So let me use nano to edit that now. All we really have to do is we have to change this last line, instead of saying false, we just have to agree to it by setting this to true. So let me change that and then we're going to click Control O to write that to that filename hit Enter and then Control X to come back out. So we're not going to try to restart the Minecraft Server yet. We're going to use a different technique in a second. What we're going to do next is we're going to create a virtual terminal screen to start that server, and to do that we're going to install screen. So let's grab that command from the lab instructions.
It seems like it was already actually installed. Then we're going to go ahead and start that now using the screen command. So let's run that and this might take a while now but it's going to establish the whole environment for us. So we can see here it's preparing the level world. It's loading some recipe. So these are all now very specific commands in regards to the gaming application that we're installing here and we're going to wait for this to complete before detaching from this and moving on. So we can see that the spawn area here has been completed. We could not detached from this, but one thing I want to point out that we're going to have to do next is when this whole thing started it told us which port it is going to do that for. So the port is right here and we're going to have to create a firewall rule in a second to actually allow client traffic to that port. So we can now detach from this. So we're going to just use Control A and Control D. To get out of here. There is a command if you wanted to reattach to the terminal, we're not going to do that. So I'm just going to exit out of here and we're now going to allow Cloud traffic. So for that, we need to create a firewall rule and we're going to use the network tag that we created which we can display by going to Columns and then Network tags. We can see that Minecraft Server was the network tag. So let's do that. I'm going to navigate to VPC network and specifically Firewall rules.
I'm going to give a new firewall rule the name of Minecraft rule. It's going to be on the default network. Could this be the only network we have right now? For specified target tags, we're now going to define Minecraft Server so only apply to the instances that have that tag. So let me define the IP ranges as from anywhere.
Now specifically for the protocol that's TCP, and then that port was 25565. Then I'm going to go ahead and click Create. Once it's up and running we're going to verify availability of the server. So I can already start navigating back and I'll monitor the process up here in the notification pane. I'm going back to Compute Engine and we have the external IP address here. We're now going to use a couple of different ways to verify that this is running. Note that we can't click on it because we didn't enable HTTP, that would have been TCP for port 80. In the lab instructions, we have listed a website and we also currently have a Chrome extension there, have that Chrome extension actually right up here. So let's try that. I'm going to go to Options, change the IP address that is in here. Save that, and then we're going to try to verify. I can change this through my Minecraft Server,
save those changes and then we're going to keep an eye on here to see if this is coming up.
Alternative also we could use any of the websites that are listed in here. Since these are third-party tools sometimes they don't work. So that's definitely something to keep in mind. I think that's actually what's going on right now. With this extension it doesn't seem to want to display this to us right now. But if I check the box in the lab instructions itself, it's telling me that everything is tracked correctly. So we've done all the work. It's just that sometimes, again, these third-party tools that we're using to test the status may not always work. There is another one that I can try really fast.
We could grab the external IP address and copy it in there.
Get the service status that way. It is telling us that it does have it, so it is up and running currently has no players in it, and it tells us the exact version that we're running. So clearly it is working for this page, just not for the Chrome extension right now. All right. So then let's move on. What we're going to do now is, these services up and running but now we want to actually scheduled some regular backups, have some maintenance around the server so that we plan for the long term. So what I can do now is I can SSH back into the server. Since I allowed for read write access to Cloud Storage, I can actually directly create a bucket now through my server here similarly as you would from Cloud Shell. So the first thing I'm going to do is I'm just going to define my own bucket name, and store it in an environment variable. So here we go, export your bucket name. You want to use something that's globally unique. So one thing we could do is we could take our project ID. You take that right here, and you go back to that server and paste it in there. Whenever you create a an environment variable, you want to run the echo command to make sure that you created it correctly. Here we can see that worked. Now I can use the gsutil command specifically MB for make bucket for Google Cloud Storage, and then use that unique part that I just entered and just append Minecraft backup so that I also know what this is. So this becomes a little bit more readable. Great. So there it is. I could also know verify by the way that it is created in my project. I could go to the navigation menu, and if we go to storage we'll be able to see our bucket right here. We could have also just created this way. But this way we now have everything stored that is the variable in here, and then going forward we can do all of the backup right through the VM. So let's go ahead, and create a backup script. I'm just going to navigate to the home directory that we have within Minecraft, and we're going to just create a new script using nano. I'm going to paste the script that we already have in here which has the screen command, and then talks about the backups. So let me paste this in there, and then we're going to press Control O, and then enter to save and control X to come back. So this script saves the current state of the server's world and pauses a service odyssey functionality. Then it's going to backup the service world data directory and place its content in a Timestamp directory in the Cloud Storage bucket. After the script, I've finished his backup the data it resumes odyssey saving on the Minecraft Server. Now we got to make sure that this is actually executable. So let's run the following command, and now we can go and test this. So let's actually run the backup script so there we can see that we are copying some files. Let's verify that. So I'm going to now navigate into my Cloud Storage bucket that I already have here. If I open that, we can now see a folder in there, and I could dig further into there to get more information about the world. So clearly we can see that the backup is working for us. We can also now schedule the backup to run in and more automated fashion. So I'm going to go back to my SSH session, run the pseudo crontab command. Now we want to choose nano in this case, it does tell us it's easiest but you do have other options available if those are more comfortable. At the bottom, we're now going to define how often this runs. This is going to tell it to run the backup every four hours. There's documentation that you can look into and how to define this, but in this case, that's more than enough for what we're trying to achieve. So let's save that file and get back out. This is going to create a lot of backups. I mean about 300 a month. So maybe you want to look into regular deleting those Cloud Storage does offer Object Lifecycle Management features that let you set the time to live for objects and even archive older objects to a different Storage class. You'll learn more about that in the next course of this series when we talk about Cloud Storage. I'm just going to go ahead and check my progress. In my lab looks like everything worked. The last thing we're going to do is now perform some maintenance. So specifically when we shut down and restart that certain actions happen. So let me run the pseudo screen command,
and then I'm going to go and actually stop this instance. So I'm going to go navigate to Compute Engine,
click on the server so select it and click stop. It's going to ask us if we sure we want to do that, and yes we're going to stop. Then later if we want to start it back up we can do that. This is also going to log us out of our SSH session obviously. So let's wait for this to stop, and then we're going to automate the server maintenance with some startup and shutdown scripts. So the instance has stopped, am going to click on it now to edit some of the custom metadata. So let me click Edit, and we're going to scroll down to the metadata. Here we go. What we're going to define now is a startup script as well as the shutdown script. We're going to point those to files that we have in Cloud Storage that are publicly available. So the key is going to be startup script URL, and then the value is going to be the location of the file. I can make them bigger to make sure that formatting that correctly. I'll add another item, and we'll do the same for the shutdown script. You can actually navigate yourself to these files if you want to, and you could read more about what exactly happens in these startup and shutdown script. So now I can click Save, and I could restart the service. I did in the meantime while the service was shutting down. I went back to the status page, and you can see that the status as currently says could not resolve so clearly the server is shut down. Now when we restart this, once all the startup script is done running, we can go back and we can verify that this service is indeed now accessible again. Just keep in mind that that might take a while for the actual instance to startup which it is now, and then for the startup script to actually finish.
