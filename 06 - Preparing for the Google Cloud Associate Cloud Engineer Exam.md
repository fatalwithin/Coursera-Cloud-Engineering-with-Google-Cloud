- [Preparing for the Google Cloud Associate Cloud Engineer Exam](#preparing-for-the-google-cloud-associate-cloud-engineer-exam)
  - [About cloud projects and accounts](#about-cloud-projects-and-accounts)
  - [About cloud projects and accounts, continued](#about-cloud-projects-and-accounts-continued)
  - [About cloud projects and accounts, continued (2)](#about-cloud-projects-and-accounts-continued-2)
  - [Billing management overview](#billing-management-overview)
  - [Command line interface](#command-line-interface)
  - [Budgeting and planning with the Pricing Calculator](#budgeting-and-planning-with-the-pricing-calculator)
  - [Planning and configuring Compute resources](#planning-and-configuring-compute-resources)
  - [Planning and configuring data storage](#planning-and-configuring-data-storage)
  - [Planning and configuring network resources](#planning-and-configuring-network-resources)
  - [Deploying and implementing compute resources](#deploying-and-implementing-compute-resources)
  - [Deploying and implementing Kubernetes resources](#deploying-and-implementing-kubernetes-resources)
  - [Deploying App Engine and Cloud Function resources](#deploying-app-engine-and-cloud-function-resources)
  - [Deploying and implementing data solutions](#deploying-and-implementing-data-solutions)
  - [Deploying and implementing networking resources](#deploying-and-implementing-networking-resources)
  - [Deploying a solution with Cloud Launcher](#deploying-a-solution-with-cloud-launcher)
  - [Deploying an application using Deployment Manager](#deploying-an-application-using-deployment-manager)
  - [Managing Compute Engine resources](#managing-compute-engine-resources)
  - [Managing Kubernetes Engine resources](#managing-kubernetes-engine-resources)
  - [Managing App Engine resources](#managing-app-engine-resources)
  - [Managing data solutions](#managing-data-solutions)
  - [Managing networking resources](#managing-networking-resources)
  - [Monitoring and logging](#monitoring-and-logging)
  - [Managing Identity and Access Management](#managing-identity-and-access-management)
  - [Defining custom IAM roles](#defining-custom-iam-roles)
  - [Managing service accounts](#managing-service-accounts)
  - [Viewing audit logs for projects and services](#viewing-audit-logs-for-projects-and-services)
  - [The path to certification](#the-path-to-certification)

## Preparing for the Google Cloud Associate Cloud Engineer Exam

### About cloud projects and accounts

Now, let's dive into the material for this module. Here the skills concerns essential for setting up new cloud projects and accounts.  
The task listed under this section involve different areas of the Google Cloud console including, the projects, user API and Stackdriver consoles.  
Creating projects and assigning users to the predefined roles of those projects are very basic first tasks. So we'll look at these first two items together.  
First, let's look at the overall structure of a simple files Google Cloud account.  
This is a diagram of an example of resource hierarchy within an organization.  
You notice that all resources are arranged under the organization node including projects. Resources can be grouped and managed together according to your organizational structure.  
You consume GCP resources by attaching them to or enabling them within a project. This is how they're built and tracked and given permissions to operate.  
Therefore, a console always have at least one project. Creating a basic project is quite an easy, but it requires some attention to labeling and naming.  
The two items highlighted are provided by the person setting up the project.  

Project ID:
- globally unique
- chosen by you
- immutable
  
Project Name:
- not unique
- chosen by you
- mutable

Project number:
- globally unique
- assigned by GCP
- immutable

Project ID once set cannot be changed. So think this one through carefully.  
The third project number is automatically generated by the GCP system and also cannot be changed.  
The main thing to remember, when it comes to organizing your projects, is that everything is arranged in a hierarchy.  
The top of this hierarchy is the organizational node. Everything else rest under this main node. Folders are used mainly for organization and after all may make a company structure.  
For example, you may find folders for marketing, finance, HR and IT within the organizations resource hierarchy.  
When you have a new organization node, by default, it lets any user and the domain create projects and billing accounts. But best practice with a new organization node is to first decide, who on your team really should be able to do those things. Then, you can assign each user to a role or roles for easier and more secure administration.  
Permissions in GCP are inheritable.  
This means that each resource that sits below another one in the hierarchy includes all of the permissions given to its parent.  
Parent permissions given to a child resource **cannot be removed** by that child or by another resource at that same level, or by anything further down in the hierarchy.  
For example, if the parent gives a user or role permission to edit a resource, a child of that parent cannot take that edit permission away.  
A child resource however, can have permissions to the ones inherited from its parent. This is how you get finer grain control over who can use or modify resources.  
What is a **role** in the context of GCP resources?  
A role is simply a collection of permissions.  
Generally, roles exist to define what **tasks** can be performed on certain **resources** and when assigned to **users**, who can perform them.
Roles help to simplify, assigning and maintain permissions on project resources.  
They assist administrators in providing everyone on the project with just the right amount of access and no more to get their jobs done.  

Three main types of roles can be applied to GCP users and resources:
- Primitive
- predefined
- custom

We will not go into custom roles in this module. Let's look at the three primitive roles first.

These primitive roles are the:
- owner
- editor
- viewer.

If you're a viewer on a given resource, you can examine it but not change its state.  
If you're an editor, you can do everything a viewer can do plus change its state and if you're an owner, you can do everything an editor can do plus manage roles and permissions on the resource.  
If you have several people working together on a project that contains sensitive data, primitive roles are probably not fine enough.   Fortunately, GCP, AIM provides other finer grain types of roles.  
**Permissions** are set to define who can do what on which resource. This is generally a very coarse way of assigning permissions in the project because it generally means assigning view, edit or administrative levels of access.  
However, GCP also offers predefined roles, which help narrow the scope down to who can do what from a long lists of unique possibilities on that particular type of resource. Each service has a predefined list of possible permissions that can be granted to users working on that resource. Predefined roles bundles selected permissions up into collections that correlate with common job-related related business needs. So instead of being granted all permissions or an ad hoc list of separate permissions, there might be missing something important.  
A user can simply be assigned a predefined role instead.  
Compute engine for example, offers a set of predefined roles that can be applied to compute engine resources in a given project, a given folder or wherever an entire organization. Let's look at this in more detail.  
In this example, we see a partial list of the compute engine permissions to have them bundled into the predefined instance admin role. The ellipsis indicate that there are more options not shown. These permissions defined what that role is allowed to do with virtual machines.   Other predefined roles in compute engine have their own customized list of permissions depending on what task that role is generally expected to perform. Having these predefined roles in place for common job functions saves time and administrated overhead.  
Google keeps these up-to-date with any new permissions that are deemed required for that role. Would it just giving everyone every permission on a servers be much simpler to maintain? Simpler, maybe but a much more risky because this violates the security principle of least privilege and it increases the chance of accidental or deliberate damage to a company's vital services or data.  
For example, an accountant might need access to reporting data generated on a compute engine instance but does not need to be able to delete that compute engine instance. Giving them permission to do so would not be good security practice, would increase the risk of accidental deletions and might in fact cause the company to violate certain regulations.  

### About cloud projects and accounts, continued

Now, let's look at the benefits of linking users in your organization to projects by their G Suite identities.  
Many new GCP customers gets started by signing into GCP console with a Gmail account. To collaborate with their teammates, these Google Groups gather people who are in the same role.  
This approach is easy to get started with, but its disadvantage is that your team's identities are not centrally managed.  
For example, if someone leaves your organization, there's no centralized way to remove their access to your Cloud resources immediately. GCP customers who are also G Suite customers can define GCP policies in terms of G Suite users in groups.  
This way, when someone leaves your organization, an administrator can immediately disable their account and remove them from groups using the Google Admin Console.  
GCP customers who are not G Suite customers can get these same capabilities through Cloud Identity.  
Cloud Identity lets you manage users and groups using the Google Admin Console, but you do not pay a received G Suite collaboration products. Another important task when setting up a new Cloud account is enabling the APIs required to develop, monitor, and report on projects.  
There are actually four ways you can interact with Google Cloud Platform. But here, we'll only talk about APIs.  
We'll discuss the Command Line Interface in the last section. The services that make up GCP offer application programming interfaces so that code you write can control them directly.  
These APIs are what's called RESTful. In other words, they follow the Representational State Transfer paradigm. We don't need to go into much detail about what that means here. But basically, it means that your code can use Google services in much the same way that web browsers talk to web servers.  
The GCP Console includes a tool called the APIs explorer that helps you learn about available APIs interactively. These API's spec parameters and documentation on parameters and how to use them is built in. You can try the APIs interactively even with user authentication.  
Suppose you've explored the methods and tried a few requests to an API, and now you're ready to build an application that uses it.  
Does that mean you have to start coding your application from zero? No. Google provides client libraries to take a lot of the drudgery out of the task of calling GCP from your code. There are two types of libraries.  
The Cloud Client Libraries are Google Cloud's latest and recommended libraries for its APIs. They adopted native styles and idioms of each language. Unfortunately, sometimes a Cloud Client Library doesn't support new services and features. In that case, you can use the Google API client library for your desired languages. These libraries are designed for generality and completeness.  

### About cloud projects and accounts, continued (2)

Last in this section is provisioning Stackdriver accounts. If you want your servers and services to remain available, you need a way to continually monitor them for performance and stability.  
One powerful option for logging and monitoring your servers and applications in Google Cloud is a product called Stackdriver.  
What is Stackdriver? Stackdriver a Multi-cloud monitoring and management service that aggregates metrics, logs, and events. It is integrated monitoring, logging, and diagnostics.  
Stackdriver manages across several platforms including GCP, AWS, and On-premises. It provides developers, operators, and security professionals with a rich set of observable signals that speed root cause analysis, and reduce mean time to resolution. Stackdriver is already integrated into many GCP services and products.  
For example Compute Engine, App Engine, Google Kubernetes Engine, Cloud SQL, Cloud Datastore, BigQuery, Networking, and Cloud Pub/sub have Stackdriver monitoring capabilities already built-in. Stackdriver logs by default are only stored for a limited number of days, the number of days depends on the type of log.  
Admin Activity audit logs are kept for 400 days which helps if you need to do forensics. Data Access audit logs are kept for only 30 days. You can export logs for analysis or longer storage. The best way to study for this section is to actually use Stackdriver. If you've never actually use Stackdriver, the Stackdriver fundamentals quests will give your hands-on experience for provisioning and using a Stackdriver account. The URL for this quest is in the resources section of this course.  

### Billing management overview

Managing budgets and bills is an important part of managing projects. The essential skills in the session include creating billing accounts, linking projects, establishing budgets and alerts, and setting up billing exports.  
To manage billing accounts and to add projects to them, you must be a **billing administrator.** To change the billing account for an existing project, you must be an **owner** on the project and the billing administrator on the destination billing account.  
When you create a new project, you're prompted to choose which of your billing accounts you want to link to the project.  
If you only have one billing account, that account is the one that will automatically link to your project.  
If you don't have a billing account, you must create one and enable billing for your project before you can use many Google Cloud platform features. Avoid surprises on your bill by creating budgets to monitor all your Google Cloud platform charges in one place. After you've set a budget amount, you set budget alert rules that are used to trigger notifications. So you can stay informed of how you're spend is tracking against your budget.  
To set a budget alert, you must be a billing administrator. You can apply budget alerts to either a billing account or project. You can set the budget to an amount you specify or match it to the previous months spent.  
Setting a budget does not cut API usage. You services will continue to operate at costs, even if the budget alert has been triggered. This last point is important, because if that cool game app you've created finally goes viral one day, the last thing you want to have is your billing alerts shut it down.  

### Command line interface

Lastly, we'll have a look at some of the things you should know about GCP's command line interface, or CLI.  
Much of this centers around the Cloud software development kit, or SDK.  
The Google Cloud platform console has a command line interface to GCP that's easily accessible from your browser. It's called Cloud Shell.  
From Cloud Shell, you can use the tools provided by the Google Cloud SDK without having to install them elsewhere. What's the SDK? We'll talk about that next. The Google Cloud SDK is a set of tools that you use to manage your resources and your applications on GCP.  
These include the gcloud tool, which provides the main command line interface for Google Cloud platform products and services. There's also gsutil, which is for Google Cloud storage, and bq, which is for BigQuery.  
The easiest way to get to the SDK commands is to click the Cloud Shell button in the TCP console. You'll get a command line interface in your web browser on a virtual machine with all these commands already installed.  
You can also install the SDK on your own computers, your laptop, your on premises servers, or virtual machines and other clouds.  
The SDK is also available as a docker image, which can be a really easy and clean way to work with it if your applications are containerized.  

### Budgeting and planning with the Pricing Calculator

Welcome to preparing for the ACE exam module three, where we'll go over planning and configuring cloud solutions. In this module, we'll cover section two of the official ACE exams study guide.  
Our first section concerns using Google Cloud's online pricing calculator. One of their first stages in setting up a new project is budgeting, because multiple variables can go into pricing a particular cloud product having a way to pull all that information together into a unified report, without having to actually configure that product first is very helpful.  
This is where Google's pricing calculator is useful. The pricing calculator is a multi-section form. When you have some possible configurations, you can use it to estimate cost for the different products you'll be using.  
The pricing calculator, it gives a total estimated cost that you can view in daily, weekly, monthly, quarterly, yearly, in three-year increments. The cost is only an estimate however, total costs may differ depending on how close to your estimated usage matches your actual usage. The estimate is also not a binding contract, it's just a planning tool. Let's look at how the planning calculator works.  

### Planning and configuring Compute resources

In the next section, we'll cover planning and configuring your project's compute resources. Compute resources are the virtual machines and servers you'll use to perform computing work. There are several different types of compute resources to choose from, and each has its strengths and special capabilities.  
We'll go over preemptible VMs in another section. So here we'll concentrate on the differences between Compute Engine, Google Kubernetes Engine, and App Engine. And how to decide which is the best computing resource for your project. In this decision chart, you'll find these three options, a list of some of their special capabilities, and some typical use cases.  
For example, if you're planning to create an app type program with no real special runtime requirements, but don't want to configure and maintain a server to support it, then App Engine is probably your best choice.  
If you need to run containerized programs or microservices on premises and one or in multiple cloud environments, or both, then Google Kubernetes Engine is your logical choice. Because it handles the orchestration of containerized apps without a requirement that they be run on a particular OS or in a particular location.  
A good way to study with this decision table is to copy it, remove all the text except for the headers, and then complete it with everything you can remember about the options shown. Then compare it to the original and you'll instantly see where you may need to do a little bit more study.

### Planning and configuring data storage

If you have computing workloads, you'll almost always have data as a requirement, or a result, and often both. What to do with that data and where to store it is a huge consideration when planning your Cloud solution.  
This section focuses on two levels of data storage concerns.  
The first-level concerns dynamic data that will be used for powering applications' reports.   You'll need to decide which type of database is the best to use when storing that data.  
The second level concerns more static data. Think of images and files here.  
Because storage and retrieval often have costs associated with them, and because it's not unusual these days for companies that have petabytes of data to maintain, these costs really can add up.  
Our first subsection concerns the differences between, and special benefits of, the major database services offered by GCP. Breaking down the information in this chart, we can see that the six data storage database options shown fall into four different categories for how the data they store is structured.  
Relational databases are the kind that most people are familiar with. These databases store information in structured columns and rows, and data is retrieved is the SQL.  
Examples of this type of database are Cloud SQL and Cloud Spanner.  
The use cases for relational databases vary, but in general if you need information on drive and e-commerce site, or content management system, you may want to choose one of the more traditional relational database options. Certain types of financial data also required a stricter structure of a relational database because that structure helps maintain data integrity.  
Non-relational databases store data in a much less structured format which is sometimes referred to as a Documents. Because of the less rigid structuring of the data, the format of incoming data can be changed over time. Perhaps by adding additional information occasionally to newer records without affecting the integrity of any older data still using previous formats.  
Example of this type of database are Cloud Datastore and Cloud Bigtable. As you can see, Non-relational database use cases have some overlap with Relational database use cases. If you know you'll need to quickly iterate, or add a lot of features to a project, using a non-relational database can allow you to keep adding fields to support new features without having to rebuild the entire database each time.  
If you expect to really heavy data traffic, Cloud Bigtable is engineered to handle that kind of load.  
Another type of database storage is the Object database, where data is stored as binary large objects or blobs. An example of this type of database is Cloud Storage. If your computing needs involves storing a lot of images or other binary media, Cloud Storage is a logical choice for your data storage needs.  
Finally, we have the option of using a Warehouse Scale Database BigQuery. BigQuery also uses a SQL, but it's built specifically to handle the huge loads that real time data streaming, analytics, and reporting can place on a database. Another consideration for data storage is, do you want to fully minute service?  
Many of Google's data storage options are fully managed. Which means you don't have to worry about upgrades or other routine minutes. You'll still need to make sure your data is backed up however. Again, making a blank copy of this table and completing it from memory is a great way to pinpoint areas you may have missed in your studies.  
The second consideration for data storage concerns choosing among various options for storing files.  
Cloud Storage let's you choose among four different types of storage classes. Regional, Multi-regional, Nearline, and Coldline. Here's one way to think about them.  
Multi-regional and Regional are for high-performance Object Storage. Nearline and Coldline are for backup and archival storage.  
Multi-regional storage is appropriate for storing frequently accessed data, Website content, interactive workloads, or data that's part of mobile and gaming applications. People use regional storage in contrast to store data close to their Compute Engine virtual Machines, or the Kubernetes Engine Clusters. This proximity gives better performance for data intensive computations.  
The availability of the storage classes varies with Multi-regional having the highest availability of 99.95 percent, followed by regional with 99.9 percent, and Nearline and Coldline with 99.9 percent.  
Retail storage lets you store your data in a specific GCP region. Us Central One, Europe West One, or Asia East One. It's cheaper than multi-regional storage, but offers less redundancy.  
Multi-regional storage costs a bit more but it's Geo-redundant. That means you can pick a broad geographic location like the United States, the European Union, or Asia and Cloud Storage stores your data in at least two geographical locations separated by at least 160 kilometers.  
Nearline storage is a low cost highly durable source service for storing infrequently accessed data. The storage class is a better choice in Multi-regional storage or Regional storage, when you plan to read or modify your data on average once a month or less.  
For example, if you want to continuously add files to Cloud Storage implied to access these files once a month for analysis, Nearline storage is a great choice. Coldline storage is a very low cost highly durable storage service for data archiving, online backup, and disaster recovery.  
Coldline storage is the best choice for data that you plan to access at most once a year due to its slightly lower availability, 90 day minimum storage duration cost for data access, and higher per operation costs.  
For example, if you want to archive data or have access in the event of a disaster recovery event.  
As for pricing, all storage classes incur a cost per gigabyte of data stored per month with Multi-regional having the highest storage price, and Coldline having lowest storage price. Egress and data transfer charges may also apply. In addition to those charges, Nearline storage also incurs an access fee per gigabyte of data read, and Coldline storage incurs a higher fee per gigabyte of data read.  

### Planning and configuring network resources

Our last subsection deals with planning and configuring network resources for your Cloud solution. When you have your compute resources established and have decided how to store your data, you need to decide how to configure access to your servers and data. We'll focus on the issue of load balancing in this last section.  
Load balancing is when you have two or more identical clusters that have been created so that if the load becomes too great or if one or more servers should fail, the remainder can assist with or take over handling the load. This is one way to create applications and services that are highly available.  
Load Balancing allows multiple service or clusters of servers to function as a single computing resource. Load balancers can also be configured to add or remove these servers or server clusters from the system to better meet demand. This is known as auto-scaling.  
If you need cross-regional load balancing for a web application use HTTP as load balancing.  
For Secure Sockets Layer traffic that is not HTTP, use the global SSL proxy load balancer.  
For other TCP traffic that does not use Secure Sockets Layer, use a global TCP proxy load balancer.  
Those two proxy services only work for specific port numbers, and they only work for TCP.  
If you want to load balance UDP traffic or traffic on any other port number, you can still load balance across a GCP region with the regional load balancer.  
Finally, all those services are intended for traffic coming into the Google network from the Internet. But what if you want to load balance traffic inside your project, say between the presentation layer and the business layer of your application?  
For that, use the internal load balancer. It accepts traffic on a GCP internal IP address and load balances across Compute Engine VMs.  
Cloud load balancer considerations can be divided up as follows: **global versus regional load balancing**, **external versus internal load balancing**, and **traffic type**.  
Use global load balancing when your users and instances are globally distributed. Your users need access to the same applications and content, and you want to provide access using a single anycast IP address.  
Global load balancing can also provide IPv6 termination.  
Use regional load balancing when your users orinstances are concentrated in one region. You only require IPv4 termination.  
Global load balancing requires you to use the premium tier of network service tiers.  
For regional load balancing, you can use the standard tier.  
GCP's load balancers can also be divided into external and internal load balancers.  
External load balancers distribute traffic coming into the Internet from your GCP network.  
Internal load balancers distribute traffic within your GCP network.  
The type of traffic your load balancer will handle is another factor in determining which load balancer to use.  
HTTP and HTTPS traffic require global external load balancing.  
TCP traffic can be handled by global external load balancing, external regional load balancing, or internal regional load balancing.  
UDP traffic can be handled by external regional load balancing or internal regional load balancing.  

### Deploying and implementing compute resources

Welcome to Module 4 of the preparing for the Associate Cloud Engineer exam. Well, we'll be discussing deploying and implementing cloud solutions.  
This section of the exam guide is a bit longer than the others, but don't fret. Here's part of what we'll be covering.  
First, let's have a look at some of the things you'll need to know about deploying and implementing Compute Engine resources.  
Here's a list of Compute Engine deployment tasks that you may encounter as questions in the certification exam. Much of this section, and many of the next few sections, test your hands-on knowledge.  
Since we can't cover everything here in this course, we'll review material on the first two sets of tasks: launching a Compute Engine instance using Cloud Console and Cloud SDK and creating an autoscaled managed instance group.  
First, let's review what Compute Engine is and what is certainly used for.  
Compute Engine lets you create and run virtual machines on Google infrastructure. They're no upfront investments, and you can run thousands of virtual CPUs on a system that is designed to be fast and offer consistent performance.  
You can create a Compute Engine virtual machine instance by using Google Cloud Platform Console or the gcloud command line tool. Your VM can run Linux and Windows Server images provided by Google or customized versions of these images.  
You can even import images from many of your physical servers. When you create a VM, you'll be asked to pick a machine type, which determines how much memory and how many virtual CPUs it has.  
This is highest range from very small to very large indeed. If you can't find a predefined type that meets your needs perfectly, you can make a custom VM. Speaking of processing power, if you have workloads like machine learning and data processing that take advantage of GPUs, many GCP zones have GPUs available for you.  
Just like physical computers need these sort of VMs, you can choose two kinds of persistent storage: standard or SSD. If your application needs high-performance scratch space, you can attach a local SSD. But be sure to store data, permanent values somewhere else, because local SSD's content won't last pass when the VM terminates.  
That's why other kinds are called persistent disk. Most people start off with standard persistent disk, and that's a default.  
There are two kinds of VM instance groups: unmanaged and managed.  
Unmanaged instance groups are collections of instances that are not necessarily identical and don't share a common instance template.  
You can use unmanaged instance groups to accommodate your pre-existing configurations for load balancing task.  
Managed instance groups allow you to operate applications on multiple identical VMs. They offer high availability, scalability using auto-scaling and automated updates.  
You should always use managed instance groups, unless your applications require you to group instances together that aren't identical.  
You can use instance templates anytime you want to quickly create VM instances based on a pre-existing configuration. Instance templates define the machine type, boot disk image or container image, labels and other instance properties.  
If you want to create a group of identical instances, you must use an instance template to create a managed instance group, which also allows you to automatically scale your number of instances.  
Instance templates are designed to create instances with identical configurations. So it's not possible to update an existing instance template or change an instance template after it's been created. If an instance template goes out of date, or you need to make changes to the configuration, create a new instance template.  

### Deploying and implementing Kubernetes resources

Next, let's have a look at some of the tasks required to deploy and implement Kubernetes Engine resources.  
In particular, the exam guys calls out the need to understand how to deploy Kubernetes Engine cluster. How to deploy containers using pods and how to configure Kubernetes Engine logging and monitoring.  
But first, let's do a bit of a review on containers and Kubernetes in general. If you aren't familiar with containers, the next few slides will give you a basic overview.  
Containers give you the independent scalability of workloads in Platform as a Service, and an abstraction layer of the OS and hardware in Infrastructure as a Service.  
Containers give you an invisible box around your code and its dependencies, with limited access to its own partition of the file system and hardware.  
It only requires a few system costs to create and starts as quickly as a process. All you need on each host is an OS kernel that supports containers in a container runtime. In essence, you're virtualizing the OS.  
It scales like Platform as a Service, but gives you nearly the same flexibility as Infrastructure as a Service. With this abstraction, your code is ultra-portable and you can treat the OS and hardware as a black box.  
So you can go from development, to staging, to production or from your laptop to the Cloud without changing or building anything.  
If you want to scale, for example, a web server, you can do so in seconds and deploy dozens or hundreds of them depending on the size of your workload on a single host. That's a simple example of scaling one container running the whole application on a single host.  
You're more likely to want to build your application using lots of containers each performing their own functions as microservices. If you build applications this way, and connect them with network connections, you can make them modular, deploy easily and scale independently across a group of hosts.  
The hosts can then scale up and down and start and stop containers as demand for your app changes or as host fail. A tool that helps you do this well is Kubernetes. Kubernetes makes it easy to orchestrate many containers on many hosts, scale them as microservices and deploy roll-outs and rollbacks.  
Now, let's have a closer look at Kubernetes Engine clusters, and why you might want to deploy a containerized application using pods. At its highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called the cluster.  
The system is divided into a set of master components that run as the control plane and a set of nodes that run containers.  
In Kubernetes, a node represents a computing instinct like a machine. In Google Cloud, nodes are virtual machines running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes figures out how to make that happen.  
Then you deploy containers on nodes using a rapper on one or more containers called a pod. A pod is the smallest unit in Kubernetes that you create or deploy.  
A pod represents a running process on your cluster as either a component of your application or an entire app.  
Generally, you only have one container per pod, but if you have multiple containers with a hard dependency, you can package them into a single pod in shared networking and storage. The pod provides a unique network IP and a set of ports for your containers, and options that govern how container should run.  
Containers inside a pod can communicate with one another using localhost and ports that remained fixed as they've started and stopped on different nodes.  
A deployment represents a group of replicas of the same pod and keeps the pods running even when nodes, they run on fail. It could represent a component of an application or an entire application. In this case, it's the nginx web server. To see the running nginx pods, run the command kubectl get pods.  

### Deploying App Engine and Cloud Function resources

Next up, App Engine and Cloud Functions.
App Engine and Cloud Functions are GCP services you use when you don't want to maintain a special server yourself. We'll first talk about App Engine.  
Other two App Engine environments, standard and flexible.  
Standard is the simpler. It offers a simpler deployment experience in a flexible environment and finer-grained auto-scaling.  
Like the standard environment, it also offers a free daily usage quota for the use of some services.  
What's distinctive about the standard environment is that low utilization applications might be able to run at no charge. Google provides App Engine software development kits in several languages so that you can test your application locally before you upload it to the real App Engine service.  
The SDKs also provides simple commands for doing the deployment.  
App Engine standard environment provides runtimes for specific versions of Java, Python, PHP, and Go.  
The runtimes also include libraries that support App Engine APIs. But for many applications, the standard environment runtimes in libraries may be all you need. But if you want to code in another language, the standard environment is not right for you, and you'll want to consider the flexible environment.  
The standard environment also enforces restrictions on your code by making it run as a so-called sandbox, that's a software construct that's independent of the hardware, operating system, or physical location of the server it runs on. The sandbox is part of why App Engine standard environment can scale a manager application in a very fine-grained way. Like all sandboxes, it imposes some constraints.  
For example, your application **can't write to the local file system**, will have to write to a database servers instead if it needs to make data persistent. Also, all the request for your application has a **60-second timeout**, and you **can't install arbitrary third party software**.  
If these constraints don't work for you, that would also be a reason to choose the flexible environment. In this diagram, we see the App Engine standard environment use. You can develop your application and run a test version of it locally using the App Engine SDK. Then when you're ready, you'll use the SDK to deploy it.  
Each App Engine application runs in a GCP project and automatically provisioned server instances, scales and load balances them. Your application can make calls to a variety of services using dedicated APIs.  
For example, a NoSQL datastore to make data persistent, caching of that data using memcache, searching, logging, user login, and the ability to launch actions not triggered by direct user requests like task use in a task scheduler.  
Let's take a few minutes now and look at Cloud Functions, what they do and how they interact with other services in GCP. Cloud Functions is a lightweight event-based asynchronous compute solution that allows you to create small single-purpose functions that respond to Cloud events without the need to manage a server or a runtime environment.  
You can use these functions to construct applications from bite-sized business logic. You can also use Cloud Functions to connect and extend Cloud services. You are billed to the nearest 100 milliseconds only while your code is running.  
Cloud Functions are written in JavaScript, Python or Go, and executed in a managed environment on Google Cloud Platform. Events from Cloud Storage and Cloud Pub/Sub can trigger Cloud Functions asynchronously, or you can use HTTP to call them synchronously.  

### Deploying and implementing data solutions

Let's now have a look at some options for handling your cloud solutions data needs. When you have applications and services, it's almost certain you will also have data.  
Therefore, knowing how to set up data services for your cloud solution is essential. Knowing how to transfer and load data into your data solution is also essential.  
However, we'll only cover data solution options in this module. There are labs in the recommended quests that will help you learn more about loading data. Let's quickly review our options for data storage and databases.  
As noted earlier in this course, data storage and database options can be divided into groups several different ways.  
One way is by how the data is structured; relational, non-relational, etc.  
Another way is by usage. With some uses potentially a good fit for more than one option. This table focuses on the technical differentiators of the storage services. Each row is a technical specification, and each column is a service.  
Let's cover each service from left to right.  
Consider using Cloud Datastore, if you need to store **structured objects**, or you'd require support for transactions in SQL like queries.  
This storage service provides terabytes of capacity with a maximum unit of one megabyte per entity.  
Consider using Cloud Bigtable If you need to store a **large amount of structured objects**. Cloud BigTable doesn't support SQL queries, nor does it support multi-row transactions. This service provides petabytes of capacity with a maximum unit of 10 megabyte per cell and 100 megabyte per row.  
Consider using Cloud Storage if you need to sort **immutable blobs larger than 10 megabytes**, such as large images or movies. This storage service provides petabytes of capacity with the maximum unit size of five terabytes per object.  
Consider using Cloud SQL or Cloud Spanner if you need **full SQL support** for an online transaction processing system. Cloud SQL provides terabytes of capacity, while Cloud Spanner provides petabytes.  
If Cloud SQL doesn't fit your requirements because you need **horizontal scalability**, consider using Cloud Spanner.  
The usual reason to store data in BigQuery is to use its big data analysis and interactive querying capabilities. You wouldn't want to use BigQuery, for example, as the back-end data store for an online application.  

### Deploying and implementing networking resources

Section 3.5 of the study guide consist implementing networking resources for your Cloud solution. Networking in resource options for your Cloud solution include VPCs, subnets, custom network configurations, IP address assignments, firewall ingress and egress rules, VPNs, and load balancing.  
In this subsection of this module, we'll focus mostly on VPC subnets. First, let's go over some VPC network review topics. Here's something that surprises a lot of people who are new to GCP.  
The Virtual Private Cloud networks that I defined have global scope.  
They can have subnets in any GCP region worldwide and subnets can span any of the zones that make up a region. You can also have resources in different zones on the same subnet.  
This architecture makes it easy for you to define your own network layout with global scope.  
The way a lot of people get started with GCP is to define their own Virtual Private Cloud inside their first GCP project, or they could simply choose the default VPC and get started with that.  
Regardless, your VPC networks connect your Google Cloud Platform resources to each other and to the Internet.  
You can segment your networks, these firewall rules restrict access to instances, and create static routes to port traffic to specific destinations. You can choose to create an automode or a custom mode VPC network.  
Each new network that you create must have a unique name within the same project. Automode networks create one subnet in each GCP region automatically when you create a network.  
As new regions become available, used subnets in those regions are automatically added to the automode network.  
IP ranges for the automatically created subnets come from a pre-determined set of ranges. All other broad networks use the same set of IP ranges. Now, let's have a look at how to implement both of these options in turn, in the following two demos.  

### Deploying a solution with Cloud Launcher

Next, we'll look at how Cloud Launcher now known as Marketplace can help you to plug cloud solutions more easily.  
In particular, we'll look at the Cloud Launcher and what it offers. Cloud Launcher provides a way to get a new server or software resource up and running quickly.  
It's a tool for quickly deploying functional software packages on Google Cloud Platform. There's no need to manually configure the software, virtual machine instances, storage, or network settings.  
Although you can modify many of them before you launch if you'd like. Most software packages in the Cloud Launcher catalog are available at no additional charge beyond the normal usage fees for GCP resources.  
Some Cloud Launcher images however do charge usage fees, particularly those published by third parties which may use commercially licensed software. But they will all show you estimates of their monthly charges before you launch them.  
Do be aware that these estimates are just but estimates, and in particular they don't attempt to estimate networking costs. Since those will vary based on how you use the applications.  
A second note of caution, GCP updates the base images for these software packages to face critical issues and vulnerabilities. But it doesn't update software after it's been deployed. Fortunately, you'll have access to the deployed systems so you can maintain them.  

### Deploying an application using Deployment Manager

The next subsection in the exam study guide is deploying an application using Deployment Manager. Specifically, we'll look at using Deployment Manager templates to provision GCP resources.  
Deployment Manager is an infrastructure management service that automates the creation and management of your Google Cloud Platform resources. Setting up your environment in GCP can entail many steps, so they have compute network and storage resources and keeping track of their configurations.  
You could do it all by hand if you want to, taking an imperative approach. But it's more efficient to use a template. That means a specification of what the environment should look like, declarative rather than imperative. GCP provides Deployment Manager to let you do just that.  
It's an infrastructure management service that automates the creation and management of your Google Cloud Platform resources.  
To use Deployment Manager, you create a template file using either the YAML Markup Language or Python, that describes what you want the components of your environment to look like.  
Then you give the template to Deployment Manager, which figures out and does the actions needed to create the environment your template describes. As you need to change your environment, edit your template, and then tell Deployment Manager to update the environment to match change. Here's a tip, you can store in version control, your Deployment Manager templates and Cloud Source Repositories.  

### Managing Compute Engine resources

Welcome to Preparing for the Associate Cloud Engineering Exam. This module will cover some concepts necessary for ensuring successful operation of a Cloud solution.  
This module is all about managing your Cloud resources, whether it's Compute Engine, Kubernetes Engine, App Engine, data solutions, network resources, or logging, and monitoring. Our first step on this particular part of the journey as managing Compute Engine Resources.  
Section 4.1 covers quite a lot of tasks needed to manage Compute Engine instances, as you can see. We won't have time in this course to go over all the tasks in this list, but let's begin by reviewing VM images in a very general way.  
You can use operating system images to create boot disks for your VM instances. You can use one of the following image types.  
**Public images** are provided and maintained by Google, open-source communities, and third-party vendors. By default, all projects have access to these images, and can use them to create instances.  
**Custom images** are available only to your project. You can create a custom image from boot disks and other images. Then use the custom image grid and instance.  
You can use most public images at no additional cost, but there are some premium images that do add additional cost to your instances. Custom images that you import to Compute Engine at no cost to your instances, but do incur an **image storage charge**, or you keep your custom image in your project. Some images are capable of running containers on Compute Engine. Support for Compute Engine provided public OS images are subject to the life cycle of the respective OS.  
Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster in a much lower cost, than if you regularly created a full image of the disk.  
Incremental snapshots work in the following manner. The first successful snapshot of a persistent disk is a full snapshot that contains all the data on a persistent disk. The second snapshot only contains any new data or modify data since the first snapshot.  
They that hasn't changed since snapshot 1 isn't included.  
Instead snapshot 2 contains references to snapshot 1 for any unchanged data.  
Snapshot 3 contains any new, or changing it since snapshot 2, but won't contain any unchanged data from snapshot 1 or 2. Instead, snapshot 3 contains references to blocks and snapshot 1 and snapshot 2 for any unchanged data. This repeats for all subsequent snapshots of the persistent disk.  
Snapshots are always created based on the last successful snapshot taken.  
Compute Engine stores multiple copies of each snapshot across multiple locations with automatic checksums to ensure the integrity of your data.  
Use IAM roles to share snapshots across project.  
To see lists of snapshots available to a project, use the `gcloud compute snapshots list` command. To list information about a particular snapshot, such as the creation time, size, and source disc, use the gcloud compute snapshots describe command.  
A custom image is the boot disk image that you own and control access to. If you regularly update your custom images with newer configurations in software, you can group those images into an image family.  
The image family always points to the most recent image in that family, so your instance templates and scripts can use that image without having to update references to a specific image version.  
You can create disk images from the following sources: a persistent disk, even while that this is attached to an instance; a snapshot of a persistent disk; another image in your project; an image that is shared from another project, or compressed RAW image in Google Cloud Storage.  

### Managing Kubernetes Engine resources

Our next section deals at managing Kubernetes Engine resources. In particular, we'll look a bit more closely at how to deploy and work with Kubernetes pods.  
A deployment represents a group of replicas of the same pod and keeps your pods running, even when nodes they run on fail.  
You could represent a component of an application or an entire App. In this case, the nginx web server by default pods and deployment are only accessible inside your GKE cluster. To make them publicly available, you can connect a load balancer to your deployment by running the `kubectl expose` command.  
Kubernetes then creates a service with a fixed type key for your pods, and the controller says, I need to attach an external load balancer with a public IP address chat service, so others outside the cluster can access it.  
In GKE, the load balancer is created as a network load balancer.  
Instead of issuing commands, you can provide a configuration file that tells Kubernetes what do you want your desired state to look like, and Kubernetes figures out how to do it. To get the file, you can run a Cube CTL get pods command as shown here to get a YAML file.  
In this case, the YAML file declares that you want three replicas of your engine next pod. It also defines a selector field, so the deployment knows how to group specific pods' replicas, and you could add a label to the pod templates, so they get selected.  
To run five replicas instead of three, all you do is update the deployment conflict file, and then run the Cube CTL apply command to use the updated config file.

### Managing App Engine resources

Our next section in the study guide concerns to management of App Engine resources, where we'll look at how to auto-scale App Engine instances.  
What are App Engine instances? Instances are the basic building blocks of App Engine, providing all the resources needed to successfully host your application.  
This includes the language runtime, the App Engine APIs, and your application's code and memory. Each instance includes a security layer to ensure that instances cannot inadvertently affect each other.  
Instances are the computing units that App Engine uses to automatically scale your application. At any given time, your application can be running on one instance or many instances with requests being spread across all of them.  
Instances are resident or dynamic.  
A dynamic instance starts up and shut down automatically based on the current needs. A resident instance runs all the time which can improve your application's performance. Both dynamic and resident instances instantiate the code included in an App Engine service version.  
If you use manual scaling for an app, the instances it runs on are resident instances. If you use either basic or automatic scaling, your app runs on dynamic instances.  
When you upload a version of a service, the app.ylm files specifies a scaling type and instance class to apply to every instance of that version. The scaling type controls how instances are created.  
The instance class determines compute resources, memory sizes and CPU speed, and pricing.  
There are three scaling types; **manual, basic, and automatic**. The available instance classes depend on the scaling type. Manual scaling uses resident instances that continuously runs a specified number of instances irrespective of the low-level.  
There's a lot of tasks such as complex initializations and applications that rely on the state of the memory over time. Automatic scaling uses dynamic instances.  
They get create a base on request rate, response latencies, and other application metrics. However, if you specify a minimum number of instances that number of instances run as residents businesses or additional instances are dynamic.  
Basic scaling uses dynamic instances. Each instance is created when the application reconsider a request. The instance will be turned down when the app becomes idle. Basic scaling is ideal for work that is intermittent or driven by user activity.

### Managing data solutions

Our next section deals with choosing the best options for managing your solution's data. In particular, we'll look at storage type options for managing cloud storage buckets.  
You can assign a life cycle management configuration to a bucket. The configuration contains a set of rules which apply to current and future objects in the bucket.  
When an object meets the criteria of one of the rules, cloud storage automatically performs its specified action on the object.  
Here are some example use cases.  
- Downgrade the storage class of objects older than 365 days to cold line storage.
- Delete objects created before January 1, 2013.
- Keep only the three most recent versions of each object in a bucket with versioning enabled.   

The following actions are supported for lifecycle: **delete, delete live and/or archive objects**. This action can be applied to both versioned and non-versioned objects.  
In a bucket with versioning enabled, deleting a live object archives the object, or deleting an archived object believes object permanently.
SetStorageClass, change the storage class of live and/or archived objects.  
This action can be applied to both versioned and non-versioned objects. The following conditions are supported for a lifecycle rule. Age, this condition is satisfied when an object reaches a specified age in days. CreatedBefore, this condition is satisfied when an object is created before midnight, the specified date and UTC is live.  
If the value is true, this life cycle conditions matches only live objects. If the value is false it matches only archive objects. MatchesStorageClass, this condition is satisfied when an object in the bucket is stored as a specified storage class. NumberOfNewerVersions, relevant only for versioned objects.  
If the value of this condition is set to n, an object satisfies the condition when there are at least n versions, including the live versions newer than n. For live objects, the number of newer versions is considered to be zero.

### Managing networking resources

Managing our Cloud solutions network resources. In this sub-session, we'll look at how to expand the CIDR block subnet. Before we begin, let's review some of the key requirements of a VPC network in the subnet.  
In a VPC network, each subnet must have a primary range in optionally up to five secondary ranges for an alias IP.  
Primary and secondary IP ranges must be RFC 1918 addresses. Within our VPC network, all primary and secondary IP addresses must be unique.  
But they don't need to be contiguous. For example, the primary arrange on a subnet can be 10.0.0.0/24 while the primary range of another subnet in the same network, can be 192.168.0.0/16.  
You can remove or replace a subnet secondary IP range only if no other instances are using that range. The primary IP range for the subnet can be expanded but not replaced or shrunk after a subnet has been created.  
The minimum primary or secondary brain size is eight IP addresses. In other words, the longest subnet mask you can use is /29.

### Monitoring and logging

Lastly, let's turn again to two very important topics; monitoring and logging. Section 4.6 covers tasks associated with monitoring your cloud solution. Making sure that it stays secure and healthy.  
Stackdriver aggregates metrics, logs, and events from your cloud infrastructure, giving you a rich set of observable data and metrics that can help developers and admins resolve security or performance issues work quickly.  
Several services including App Engine flexible, App Engine standard, and Kubernetes Engine have Stackdriver Monitoring built-in.  
For other services without Stackdriver Monitoring built-in, such as Compute Engine, there are monitoring engines that can be installed.  
Stackdriver Monitoring engines even exist for Amazon EC2 and non-cloud services. This allows Stackdriver to provide a multi-cloud hybrid monitoring solution. You can also create and manage alerting policies with the Stackdriver Monitoring console, the Stackdriver Monitoring API and the Cloud SDK.  
Each policy specifies the following: conditions that identify an unhealthy state for a resource or a group of resources, optional notifications set their email, SMS or other channels to let your support team know that a resource is unhealthy.  
Optional documentation that can be included in some types of notifications to help your support team resolve the issue. When events trigger conditions in one of your alerting policies, Stackdriver Monitoring creates and displays an incident in the Stackdriver Monitoring console.  
If you set up notifications, Stackdriver Monitoring also sends notifications to people or third party notification services. Respondents can acknowledge receipt of the notification, but the incident remains open until resources are no longer in an unhealthy state.  
In the site reliability troubleshooting with Stackdriver APM lab in our final module, you'll have a chance to see Stackdriver at work and gain hands-on experience setting it up.

### Managing Identity and Access Management

Welcome back to preparing for the associate cloud engineer exam.  
In this module, we'll discuss how to configure your cloud solution access and security. Our first section concerns managing identity and access management, also known as IAM.  
Tasks we'll cover in this section include viewing and assigning IAM roles to accounts or Google groups. First, let's review some basic information about cloud IAM.
Access is granted to members which can be a Google account representing a developer, administrator, or any other person who interacts with DCP.  
A service account, which is an account that belongs to your application, is seven individual end user.  
A Google group, which is a named collection of Google accounts and service accounts, and each has a unique email address associated with that group.  
A G Suite domain representing a virtual group of all the Google accounts have been created in an organization's G Suit account, Or a cloud identity domain. Which is like a G Suit domain because it represents a virtual group of all Google accounts in an organization.  
However, Cloud Identity domain users don't have access to these three applications and features. To view IAMs, open the IAM page in the GCC console, click Select a Project, and then click Open. The page will then display a list of members of the project and their roles.  
What if you need to add a new member to the project, or change member roles, or revoke access to a project? Let's watch a demo of those processes next.

### Defining custom IAM roles

Next, let's have a look at custom roles in IAM. In addition to the pre-defined roles, Cloud IAM provides the ability to create customized Cloud IAM roles. To create a custom role, you need to know what permissions are available for the resource you wish to grant them on.  
You can use this information to create a custom Cloud IAM role with one or more permissions. And then grant that custom role to users who are part of your organization. Role metadata is how you find out what permissions can be granted. It can be found by using the Google Cloud platform console or the IAM API.  
Once you've decided what to call your role and what permissions to give it, creating the role and adding permissions is fairly simple as long as you have the iam.roles.create permission on your user account or role. You can also create a custom role using the curated role as it's base.  
This means you'd take a role that is similar to one you need to create. And then add or remove permissions from a copy of that role until it meets your needs exactly. We'll create a custom role in the next demo.  

### Managing service accounts

Our next the same guide session is managing service accounts. We'll now discuss managing these special counsel limited scopes.  
First let's go over what a service account is.  
A service account is a special Google account that belongs to your application or a VM and is set up to an individual end user.  
Your application uses the service account to call the Google API of a service so that the users aren't directly involved. For example, a compute engine VM may run as a service account, and that account can be given permissions to access resources it needs.  
This way the service account is the identity of the service, and the service account's permissions control which resources the service can access.  
A service account is identified by its email address, which is unique to the account. Access scopes are the legacy method of specifying permissions for your VM. Before these systems of IM roles, action scopes were the only mechanism for granting permissions to service accounts.  
Although they aren't the primary way of granting permissions now, you must still set access scopes when configuring an instance to run as a service account.  
In addition the permissions granted in the role with the scope must agree. If they don't, the service account won't be able to perform the function you need it to. The scope consists of the base URL up to the auth section plus the specific permission being granted, as shown in the example here.  
Scope can also be set on the command line using set scopes with the GCloud command. A VMs instance can only perform operations that are allowed by the roles assigned to the service account and the scopes that have been defined on the instance.  
And those permissions cannot contradict.  
For example, if a role only grants view access to our resource but a scope allows edit access, then the instance will not be able to edit that resource.  
To enable edit access to the resource, the role would need to be modified so that it agreed with the permissions granted in the scope. In other words, it would need to be changed to allow editing.  
If you need to change access scopes on an instance, you'll need to stop that instance first and then restart it for the changes to take effect.  
You can grant different groups of VMs in your project different identities. This makes it easier to manage different permissions for each group. You can also change the permissions of the service accounts without having to recreate the VMs.  
In the examples seen here, VMs running component 1 are granted editor access to project_b using Service Account 1.  
VMs running component_2 are granted objectViewer access to bucket_1 using Service Account 2.  
Service account permissions can also be changed without recreating VMs. Here's a more complex example. Say you have an application that's implemented across a group of compute engine virtual machines.  
One component of your application needs to have an editor role on another project, but another component doesn't.  
You would need to create two different service accounts, one for each sub group of virtual machines. Only the first service account has privilege on the other project.  
That reduces the potential impact of a miscoded application or a compromised virtual machine. Now, let's look at how you can assign and grant access to a service account in the same project or a different project.  
If you want to run the VM as a different identity, or you determine that the instance needs a different set of scopes to call the required API's.  
You can change the service account and the access scopes of an existing instance.  
For example, you can change access scopes or grant access to a new API or change an instance so that it runs as a service account that you created instead of the compute engine default service account.  
To change an instance's service account and access scopes, the instance must be temporarily stopped.  
To stop the instance, read the documentation for stopping instances. After changing the service account or access scopes, remember to restart the instance. You'll now gain some hands on experience managing and using service accounts in the next lab.

### Viewing audit logs for projects and services

Lastly, let's have a look at using audit logs for projects and manage services.  
Audit logs are very valuable resources for managing security and maintaining your Cloud services.  
Cloud audit logging means has three audit logs for each project folder and organization: **Admin activity, system event, and data access**.  
Google Cloud platform services right audit log entries to these logs to help you answer the questions of who did what? Where? And when? Within your GCP projects. These logs contain the following information.  
Resource. Each audit log entry includes a resources some type. For example, you can view audit log entries from a seal Compute VM instance or from VM instance's.  
Service. Services are individual GCP products. Such as Compute engine, Cloud SQL or Cloud Pub Sub. Each service is identified by name. Compute engine is compute.googleapis.com. Cloud SQL is Cloud SQL.googleapis.com and so-forth.  
How audit logs can be viewed through the sacked ever interface from the main Console menu? The view audit logs on a VM instance within a project, navigate to the Stackdriver section of the main Console menu. You'll see options for Stackdriver components listed under the main heading.  
Choose Logging and then Logs if you'd log entries for instance. You can view abbreviated project level audit logs and your projects activity page in the GCP console.  
Navigate to the Home, activity page, and then you filter to select the entries you want to see. As mentioned, these entries are abbreviated. So the actual audit log entries might contain more information than you see in the activity page.

### The path to certification

Hi. I'm Katie Richardson, a Program Manager on the Google Cloud Certifications team. In this video, I'll introduce you to the Google Cloud Certification Program and share why certification might be right for you. Google Cloud is the fastest growing cloud, with significant advantages in big data processing and analytics at scale. Whether you are just beginning your career in Cloud, or have more experience with other Cloud solutions, validating your technical expertise by getting certified on Google Cloud will help demonstrate that you have the skills critical to drive business success in today's highly competitive market. Certification is essential in today's market to validate mastery and competency of technologies that are critical to business success.  
For individuals, certification provides credibility and a testament to your skills and proficiency in Google Cloud Platform. It also provides industry recognition. A certification is a formal recognition of professionals who meet established proficiency standards. Career advancement. Certifications can give you a competitive advantage over someone who has not certified. Finally, personal and professional development. Certifications ensure that you stay up-to-date with industry and technology developments.  
We also have a Google Cloud certified community that provides opportunities to network and exchange ideas. Let's take a look at what some of our customers and partners are saying about the value of certification.
What certification offers that experience doesn't is peace of mind. I'm not only talking about self-confidence, but also for our customers. Having us certified working on their projects really gives them peace of mind that they're working with a partner who really knows what they're doing.  
Certification proves that an engineer not only has that real-world experience, but also has the technical knowledge to be able to be certified and to operate at the level that is required. The Google Cloud Certification Program includes two levels of certifications, professional and associate.  
The associate certification is focused on core Google Cloud Platform technology and is a good starting point for those new to Cloud or new to Google Cloud Platform. Achieving this certification can also help you on your path to professional certifications, by validating your core technical skills that will help you be successful in more advanced or specialized job roles.  
The professional certifications are job role-based certifications and assess advanced design and implementation skills, developed through hands-on experience. These certifications are focused on the core job roles in Cloud infrastructure, data and machine learning, application development, networking and security.  
Now that we've covered the basics of our program, let's talk about the steps to prepare for a Google Cloud Certification. Your professional and your classroom experience is the most critical part of your preparation journey. Use the exam guide to gauge how your skills and knowledge might match up to the job skills that will be evaluated in the certification exam.  
You can also use training and hands-on labs to refresh your knowledge and get additional hands-on experience. We also recommend taking the practice exams that are available for our certifications. These practice exams will familiarize you with the types of questions you may encounter on certification exam and help you determine your readiness or if you need more preparation or experience.  
You can find all of these resources on the Google Cloud Certification website at cloud.google.com/certification. We also recommend checking out some of the additional resources, like our cloud documentation pages and the Google Cloud Platform YouTube channel for videos and tutorials on Google Cloud products and solutions. Visit the Google Cloud Certification website for more information and take the first step to becoming Google Cloud certified.

